{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipCI dataset\n",
    "columns = ['ci_skipped', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev',\n",
    "       'age', 'nuc', 'exp', 'rexp', 'sexp', 'TFC', 'is_doc', 'is_build',\n",
    "       'is_meta', 'is_media', 'is_src', 'is_merge', 'FRM', 'COM', 'CFT',\n",
    "       'classif', 'prev_com_res', 'proj_recent_skip', 'comm_recent_skip',\n",
    "       'same_committer', 'is_fix', 'day_week', 'CM', 'commit_hash']\n",
    "\n",
    "path = '/content/drive/MyDrive/CI/SkipCI-dataset'\n",
    "path = '/mnt/d/PFE/Papers Presentations/1SkipCI/SkipCI/dataset/'\n",
    "\n",
    "# projects list: \n",
    "# candybar-library.csv  GI.csv               mtsar.csv     ransack.csv     SemanticMediaWiki.csv\n",
    "# contextlogger.csv     grammarviz2_src.csv  parallec.csv  SAX.csv         solr-iso639-filter.csv\n",
    "# future.csv            groupdate.csv        pghero.csv    searchkick.csv  steve.csv\n",
    "\n",
    "valid_proj = 'SemanticMediaWiki.csv'\n",
    "cols_to_keep = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, X_val, y_train, y_val, df, eval_meth):\n",
    "    print(eval_meth, m)\n",
    "    if m=='rf':\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "    if m=='dt':\n",
    "        model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Classification metrics calculations\n",
    "    #report = classification_report(y_val, y_pred)\n",
    "    #confusion = confusion_matrix(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    #print(report)\n",
    "    #print('Confusion Matrix')\n",
    "    #print(confusion)\n",
    "    print('F1=%.3f' % (f1))\n",
    "    print('AUC=%.3f\\n' % (auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_eval(valid_proj):\n",
    "    df = pd.read_csv(os.path.join(path, valid_proj))\n",
    "    X = df.iloc[:,1:cols_to_keep]\n",
    "    y = df.iloc[:,0].astype(int)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val , y_train, y_val = train_test_split(np.array(X), np.array(y), test_size=0.2, shuffle=True, stratify=y, random_state=42) # keep ratio of classes in split\n",
    "\n",
    "    eval_meth = f'within_proj_{valid_proj}'[:-4]\n",
    "    \n",
    "    train(X_train, X_val, y_train, y_val, df, eval_meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def cross_eval(valid_proj, oversample=0):\n",
    "\n",
    "    df_train = pd.DataFrame(columns=columns, dtype='object')\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if filename[-4:]==\".csv\" and filename!=valid_proj:\n",
    "                df_train = pd.concat([df_train, pd.read_csv(os.path.join(dirname, filename))])\n",
    "\n",
    "    X_train = np.array(df_train.iloc[:,1:cols_to_keep])\n",
    "    y_train = np.array(df_train.iloc[:,0].astype(int))\n",
    "\n",
    "    df_val = pd.read_csv(os.path.join(path, valid_proj))\n",
    "    df = df_val \n",
    "\n",
    "    X_val = np.array(df_val.iloc[:,1:cols_to_keep])\n",
    "    y_val = np.array(df_val.iloc[:,0].astype(int))\n",
    "\n",
    "    eval_meth = f'cross_proj_{valid_proj}'[:-4]\n",
    "\n",
    "    if oversample:\n",
    "        oversample = SMOTE()\n",
    "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "        X_val, y_val = oversample.fit_resample(X_val, y_val)\n",
    "\n",
    "    train(X_train, X_val, y_train, y_val, df, eval_meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within_proj_mtsar rf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80        52\n",
      "           1       0.62      0.48      0.54        27\n",
      "\n",
      "    accuracy                           0.72        79\n",
      "   macro avg       0.69      0.66      0.67        79\n",
      "weighted avg       0.71      0.72      0.71        79\n",
      "\n",
      "Confusion Matrix\n",
      "[[44  8]\n",
      " [14 13]]\n",
      "\n",
      "F1=0.542\n",
      "\n",
      "AUC=0.664\n"
     ]
    }
   ],
   "source": [
    "m='rf'\n",
    "within_eval(\"mtsar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_proj_pghero rf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89       503\n",
      "           1       0.94      0.81      0.87       503\n",
      "\n",
      "    accuracy                           0.88      1006\n",
      "   macro avg       0.89      0.88      0.88      1006\n",
      "weighted avg       0.89      0.88      0.88      1006\n",
      "\n",
      "Confusion Matrix\n",
      "[[479  24]\n",
      " [ 98 405]]\n",
      "\n",
      "F1=0.869\n",
      "\n",
      "AUC=0.879\n"
     ]
    }
   ],
   "source": [
    "m='rf'\n",
    "cross_eval(\"pghero.csv\", oversample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within_proj_candybar-library rf\n",
      "F1=0.833\n",
      "AUC=0.688\n",
      "\n",
      "within_proj_GI rf\n",
      "F1=0.824\n",
      "AUC=0.921\n",
      "\n",
      "within_proj_mtsar rf\n",
      "F1=0.542\n",
      "AUC=0.664\n",
      "\n",
      "within_proj_ransack rf\n",
      "F1=0.821\n",
      "AUC=0.857\n",
      "\n",
      "within_proj_SemanticMediaWiki rf\n",
      "F1=0.047\n",
      "AUC=0.503\n",
      "\n",
      "within_proj_contextlogger rf\n",
      "F1=1.000\n",
      "AUC=1.000\n",
      "\n",
      "within_proj_grammarviz2_src rf\n",
      "F1=0.778\n",
      "AUC=0.818\n",
      "\n",
      "within_proj_parallec rf\n",
      "F1=1.000\n",
      "AUC=1.000\n",
      "\n",
      "within_proj_SAX rf\n",
      "F1=0.848\n",
      "AUC=0.882\n",
      "\n",
      "within_proj_solr-iso639-filter rf\n",
      "F1=0.892\n",
      "AUC=0.907\n",
      "\n",
      "within_proj_future rf\n",
      "F1=1.000\n",
      "AUC=1.000\n",
      "\n",
      "within_proj_groupdate rf\n",
      "F1=0.694\n",
      "AUC=0.790\n",
      "\n",
      "within_proj_pghero rf\n",
      "F1=0.778\n",
      "AUC=0.850\n",
      "\n",
      "within_proj_searchkick rf\n",
      "F1=0.798\n",
      "AUC=0.847\n",
      "\n",
      "within_proj_steve rf\n",
      "F1=0.211\n",
      "AUC=0.564\n",
      "\n",
      "cross_proj_candybar-library rf\n",
      "F1=0.539\n",
      "AUC=0.612\n",
      "\n",
      "cross_proj_GI rf\n",
      "F1=0.805\n",
      "AUC=0.863\n",
      "\n",
      "cross_proj_mtsar rf\n",
      "F1=0.375\n",
      "AUC=0.592\n",
      "\n",
      "cross_proj_ransack rf\n",
      "F1=0.556\n",
      "AUC=0.708\n",
      "\n",
      "cross_proj_SemanticMediaWiki rf\n",
      "F1=0.196\n",
      "AUC=0.530\n",
      "\n",
      "cross_proj_contextlogger rf\n",
      "F1=0.976\n",
      "AUC=0.977\n",
      "\n",
      "cross_proj_grammarviz2_src rf\n",
      "F1=0.784\n",
      "AUC=0.844\n",
      "\n",
      "cross_proj_parallec rf\n",
      "F1=0.913\n",
      "AUC=0.909\n",
      "\n",
      "cross_proj_SAX rf\n",
      "F1=0.782\n",
      "AUC=0.860\n",
      "\n",
      "cross_proj_solr-iso639-filter rf\n",
      "F1=0.637\n",
      "AUC=0.698\n",
      "\n",
      "cross_proj_future rf\n",
      "F1=0.427\n",
      "AUC=0.635\n",
      "\n",
      "cross_proj_groupdate rf\n",
      "F1=0.566\n",
      "AUC=0.708\n",
      "\n",
      "cross_proj_pghero rf\n",
      "F1=0.471\n",
      "AUC=0.655\n",
      "\n",
      "cross_proj_searchkick rf\n",
      "F1=0.543\n",
      "AUC=0.689\n",
      "\n",
      "cross_proj_steve rf\n",
      "F1=0.161\n",
      "AUC=0.544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m='rf'\n",
    "\n",
    "for valid_proj in ['candybar-library.csv','GI.csv', 'mtsar.csv', 'ransack.csv', 'SemanticMediaWiki.csv', 'contextlogger.csv', 'grammarviz2_src.csv', 'parallec.csv', 'SAX.csv', 'solr-iso639-filter.csv', 'future.csv', 'groupdate.csv', 'pghero.csv', 'searchkick.csv', 'steve.csv']:\n",
    "    within_eval(valid_proj)\n",
    "for valid_proj in ['candybar-library.csv','GI.csv', 'mtsar.csv', 'ransack.csv', 'SemanticMediaWiki.csv', 'contextlogger.csv', 'grammarviz2_src.csv', 'parallec.csv', 'SAX.csv', 'solr-iso639-filter.csv', 'future.csv', 'groupdate.csv', 'pghero.csv', 'searchkick.csv', 'steve.csv']:\n",
    "    cross_eval(valid_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
