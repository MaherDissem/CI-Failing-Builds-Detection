{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipCI dataset\n",
    "columns = ['ci_skipped', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev',\n",
    "       'age', 'nuc', 'exp', 'rexp', 'sexp', 'TFC', 'is_doc', 'is_build',\n",
    "       'is_meta', 'is_media', 'is_src', 'is_merge', 'FRM', 'COM', 'CFT',\n",
    "       'classif', 'prev_com_res', 'proj_recent_skip', 'comm_recent_skip',\n",
    "       'same_committer', 'is_fix', 'day_week', 'CM', 'commit_hash']\n",
    "\n",
    "path = '/content/drive/MyDrive/CI/SkipCI-dataset'\n",
    "path = '/mnt/d/PFE/Papers Presentations/1SkipCI/SkipCI/dataset/'\n",
    "\n",
    "# projects list: \n",
    "# candybar-library.csv  GI.csv               mtsar.csv     ransack.csv     SemanticMediaWiki.csv\n",
    "# contextlogger.csv     grammarviz2_src.csv  parallec.csv  SAX.csv         solr-iso639-filter.csv\n",
    "# future.csv            groupdate.csv        pghero.csv    searchkick.csv  steve.csv\n",
    "\n",
    "valid_proj = 'SemanticMediaWiki.csv'\n",
    "cols_to_keep = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_eval(valid_proj):\n",
    "    df = pd.read_csv(os.path.join(path, valid_proj))\n",
    "    X = df.iloc[:,1:cols_to_keep]\n",
    "    y = df.iloc[:,0].astype(int)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val , y_train, y_val = train_test_split(np.array(X), np.array(y), test_size=0.2, shuffle=True, stratify=y, random_state=42) # keep ratio of classes in split\n",
    "\n",
    "    eval_meth = f'within_proj_{valid_proj}'[:-4]\n",
    "\n",
    "    train(X_train, X_val, y_train, y_val, df, eval_meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_eval(valid_proj):\n",
    "\n",
    "    df_train = pd.DataFrame(columns=columns, dtype='object')\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if filename[-4:]==\".csv\" and filename!=valid_proj:\n",
    "                df_train = pd.concat([df_train, pd.read_csv(os.path.join(dirname, filename))])\n",
    "\n",
    "    X_train = np.array(df_train.iloc[:,1:cols_to_keep])\n",
    "    y_train = np.array(df_train.iloc[:,0].astype(int))\n",
    "\n",
    "    df_val = pd.read_csv(os.path.join(path, valid_proj))\n",
    "    df = df_val \n",
    "\n",
    "    X_val = np.array(df_val.iloc[:,1:cols_to_keep])\n",
    "    y_val = np.array(df_val.iloc[:,0].astype(int))\n",
    "\n",
    "    eval_meth = f'cross_proj_{valid_proj}'[:-4]\n",
    "\n",
    "    train(X_train, X_val, y_train, y_val, df, eval_meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, X_val, y_train, y_val, df, eval_meth):\n",
    "    print(eval_meth, m)\n",
    "    if m=='rf':\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "    if m=='dt':\n",
    "        model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Classification metrics calculations\n",
    "    report = classification_report(y_val, y_pred)\n",
    "    confusion = confusion_matrix(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    print(report)\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion)\n",
    "    print('\\nF1=%.3f' % (f1))\n",
    "    print('\\nAUC=%.3f' % (auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within_proj_mtsar rf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80        52\n",
      "           1       0.62      0.48      0.54        27\n",
      "\n",
      "    accuracy                           0.72        79\n",
      "   macro avg       0.69      0.66      0.67        79\n",
      "weighted avg       0.71      0.72      0.71        79\n",
      "\n",
      "Confusion Matrix\n",
      "[[44  8]\n",
      " [14 13]]\n",
      "\n",
      "F1=0.542\n",
      "\n",
      "AUC=0.664\n"
     ]
    }
   ],
   "source": [
    "m='rf'\n",
    "within_eval(\"mtsar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_proj_SemanticMediaWiki rf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87      6623\n",
      "           1       0.24      0.17      0.20      1315\n",
      "\n",
      "    accuracy                           0.77      7938\n",
      "   macro avg       0.54      0.53      0.53      7938\n",
      "weighted avg       0.74      0.77      0.76      7938\n",
      "\n",
      "Confusion Matrix\n",
      "[[5928  695]\n",
      " [1097  218]]\n",
      "\n",
      "F1=0.196\n",
      "\n",
      "AUC=0.530\n"
     ]
    }
   ],
   "source": [
    "m='rf'\n",
    "cross_eval(\"SemanticMediaWiki.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for valid_proj in ['candybar-library.csv','GI.csv', 'mtsar.csv', 'ransack.csv', 'SemanticMediaWiki.csv', 'contextlogger.csv', 'grammarviz2_src.csv', 'parallec.csv', 'SAX.csv', 'solr-iso639-filter.csv', 'future.csv', 'groupdate.csv', 'pghero.csv', 'searchkick.csv', 'steve.csv']:\n",
    "    within_eval(valid_proj)\n",
    "    #cross_eval(valid_proj)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
