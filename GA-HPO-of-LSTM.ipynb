{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install optunity &> /dev/null\n",
    "!pip install ConfigSpace &> /dev/null\n",
    "!pip install hpbandster &> /dev/null\n",
    "!pip install hyperopt &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class that represents the solution to be evolved.\"\"\"\n",
    "import random\n",
    "class Solution():\n",
    "    def __init__(self, all_possible_params):\n",
    "        self.entry = {}\n",
    "        self.score = 0.\n",
    "        self.all_possible_params = all_possible_params\n",
    "        self.params = {}  #  represents model parameters to be picked by creat_random method\n",
    "        self.model = None\n",
    "        \n",
    "    \"\"\"Create the model random params.\"\"\"\n",
    "    def create_random(self):\n",
    "        for key in self.all_possible_params:\n",
    "            self.params[key] = random.choice(self.all_possible_params[key])\n",
    "\n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "      \n",
    "    \"\"\"\n",
    "        Train the model and record the score.\n",
    "    \"\"\"\n",
    "    def train_model(self, fn_train,params_fn):\n",
    "        \n",
    "        if self.score == 0.:\n",
    "                res = fn_train(self.params,params_fn)\n",
    "                self.score =  res[\"entry\"][\"F1\"] #1-float(res[\"validation_loss\"])\n",
    "                self.model = res[\"model\"]\n",
    "                self.entry = res['entry']\n",
    "            \n",
    "    \"\"\"Print out a network.\"\"\"\n",
    "    def print_solution(self):\n",
    "        print(\"for params \", self.params , \"the score in the train = \",self.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class that holds a genetic algorithm for evolving a population of params.\n",
    "\"\"\"\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import random\n",
    "\"\"\"Class that implements genetic algorithm for Hyper-parameter tuning\"\"\"\n",
    "class Optimizer():\n",
    "    \n",
    "    def __init__(self, GA_params, all_possible_params):\n",
    "        \"\"\"Create an optimizer.\"\"\"\n",
    "        self.random_select = GA_params[\"random_select\"]\n",
    "        self.mutate_chance = GA_params[\"mutate_chance\"]\n",
    "        self.retain = GA_params[\"retain\"]\n",
    "        self.all_possible_params = all_possible_params\n",
    "    \n",
    "    def create_population(self, count):\n",
    "        \"\"\"Create a population of random solutions.\"\"\"\n",
    "        pop = []\n",
    "        for _ in range(0, count):\n",
    "            # Create a random solution.\n",
    "            solution = Solution(self.all_possible_params)\n",
    "            solution.create_random()\n",
    "            # Add the solution to our population.\n",
    "            pop.append(solution)\n",
    "        return pop\n",
    "\n",
    "    @staticmethod\n",
    "    def fitness(solution):\n",
    "        \"\"\"Return the score, which is our fitness function.\"\"\"\n",
    "        return solution.score\n",
    "\n",
    "    def grade(self, pop):\n",
    "        \"\"\"Find average fitness for a population. \"\"\"\n",
    "        summed = reduce(add, (self.fitness(solution) for solution in pop))\n",
    "        return summed / float((len(pop)))\n",
    "\n",
    "    def crossover(self, mother, father):\n",
    "        \"\"\"Make two children as parts of their parents.\n",
    "        Args:\n",
    "            mother (dict): parameters\n",
    "            father (dict): parameters\n",
    "        Returns:\n",
    "            (list): combined params\n",
    "        \"\"\"\n",
    "        children = []\n",
    "        for _ in range(2):\n",
    "            child = {}\n",
    "            # Loop through the parameters and pick params for the kid.\n",
    "            for param in self.all_possible_params:\n",
    "                child[param] = random.choice([mother.params[param], father.params[param]] )\n",
    "\n",
    "            solution = Solution(self.all_possible_params)\n",
    "            solution.set_params(child)\n",
    "            # Randomly mutate some of the children.\n",
    "            if self.mutate_chance > random.random():\n",
    "                solution = self.mutate(solution)\n",
    "            children.append(solution)\n",
    "        return children\n",
    "    \n",
    "    \n",
    "    def mutate(self, solution):\n",
    "        \"\"\"Randomly mutate one part of the solution.\"\"\"\n",
    "        # Choose a random key.\n",
    "        mutation = random.choice(list(self.all_possible_params.keys()))\n",
    "        # Mutate one of the params.\n",
    "        solution.params[mutation] = random.choice(self.all_possible_params[mutation])\n",
    "        return solution\n",
    "    \n",
    "    \"\"\"Evolve a population of solutions.\"\"\"\n",
    "    def evolve(self, pop):\n",
    "        #Get scores for each solution.\n",
    "        graded = [(self.fitness(solution), solution) for solution in pop]\n",
    "        #\"Sort on the scores.\n",
    "        graded = [x[1] for x in sorted(graded, key=lambda x: x[0], reverse=True)]\n",
    "        #Get the number we want to keep for the next gen.\n",
    "        retain_length = int(len(graded)*self.retain)\n",
    "        # define what we want to keep.\n",
    "        parents = graded[:retain_length]\n",
    "        # For those we aren't keeping, randomly keep some anyway.\n",
    "        for individual in graded[retain_length:]:\n",
    "            if self.random_select > random.random():\n",
    "                parents.append(individual)\n",
    "        # Now find out how many spots we have left to fill.\n",
    "        parents_length = len(parents)\n",
    "        desired_length = len(pop) - parents_length\n",
    "        \n",
    "        # Add children, which are bred from two remaining solutions.\n",
    "        if parents_length > 1 and desired_length> 0:\n",
    "            children = []\n",
    "            while len(children) < desired_length:\n",
    "                if parents_length==2:\n",
    "                    male_index = 1\n",
    "                    female_index = 0\n",
    "                else:\n",
    "                    male_index = random.randint(0, parents_length-1)\n",
    "                    female_index = random.randint(0, parents_length-1)\n",
    "                \n",
    "                # Assuming they aren't the same solutions...\n",
    "                if male_index != female_index:\n",
    "                    print(\"Get a random mom and dad.\")\n",
    "                    male = parents[male_index]\n",
    "                    female = parents[female_index]\n",
    "                    # crossover them.\n",
    "                    babies = self.crossover(male, female)\n",
    "                    # Add the children one at a time.\n",
    "                    for baby in babies:\n",
    "                        # Don't grow larger than desired length.\n",
    "                        if len(children) < desired_length:\n",
    "                            children.append(baby)\n",
    "            parents.extend(children)\n",
    "        return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import threading\n",
    "def train_sol_thread(solution,fn_train,params_fn,i):\n",
    "    solution.train_model(fn_train,params_fn)\n",
    "    print(\"solution \", i,\" trained\")\n",
    "    \n",
    "def train_population(pop, fn_train,params_fn):\n",
    "    pbar = tqdm(total=len(pop))\n",
    "    threads = list()\n",
    "    i=1\n",
    "    for solution in pop:\n",
    "        x = threading.Thread(target=train_sol_thread, args=(solution,fn_train,params_fn,i))\n",
    "        i=i+1\n",
    "        threads.append(x)\n",
    "        x.start()\n",
    "        pbar.update(1)\n",
    "        \n",
    "    for index, thread in enumerate(threads):\n",
    "        thread.join()\n",
    "    pbar.close()\n",
    "\n",
    "\n",
    "def get_average_score(pop):\n",
    "    \"\"\"Get the average score for a group of solutions.\"\"\"\n",
    "    total_scores = 0\n",
    "    for solution in pop:\n",
    "        total_scores += solution.score\n",
    "    return total_scores / len(pop)\n",
    "\n",
    "\n",
    "def generate(all_possible_params, fn_train , params_fn):\n",
    "    \"\"\"Generate the optimal params with the genetic algorithm.\"\"\"\n",
    "    \"\"\" Args:\n",
    "            GA_params: Params for GA\n",
    "            all_possible_params (dict): Parameter choices for the model\n",
    "            train_set : training dataset\n",
    "            fn_train : a function used to compute the prediction accuracy\n",
    "    \"\"\"\n",
    "   \n",
    "    GA_params = {\n",
    "            \"population_size\": nbr_sol,\n",
    "            \"max_generations\": nbr_gen,\n",
    "            \"retain\": 0.7,\n",
    "            \"random_select\":0.1,\n",
    "            \"mutate_chance\":0.1\n",
    "            }\n",
    "    \n",
    "    print(\"params of GA\" , GA_params)\n",
    "    optimizer = Optimizer(GA_params ,all_possible_params)\n",
    "    pop = optimizer.create_population(GA_params['population_size'])\n",
    "    # Evolve the generation.\n",
    "    for i in range(GA_params['max_generations']):\n",
    "        print(\"*********************************** REP(GA) \",(i+1))\n",
    "        # Train and get accuracy for solutions.\n",
    "        train_population(pop,fn_train,params_fn)\n",
    "        # Get the average accuracy for this generation.\n",
    "        average_accuracy = get_average_score(pop)\n",
    "        # Print out the average accuracy each generation.\n",
    "        print(\"Generation average: %.2f%%\" % (average_accuracy * 100))\n",
    "        # Evolve, except on the last iteration.\n",
    "        if i != (GA_params['max_generations']):\n",
    "            print(\"Generation evolving..\")\n",
    "            evolved = optimizer.evolve(pop)\n",
    "            if(len(evolved)!=0):\n",
    "                pop=evolved\n",
    "        else:\n",
    "            pop = sorted(pop, key=lambda x: x.score, reverse=True)\n",
    "    # Print out the top 2 solutions.\n",
    "    size = len(pop)\n",
    "    if size < 3:\n",
    "        print_pop(pop[:size])\n",
    "    else:\n",
    "        print_pop(pop[:3])\n",
    "    return pop[0].params ,pop[0].model,pop[0].entry\n",
    "\n",
    "def print_pop(pop):\n",
    "    for solution in pop:\n",
    "        solution.print_solution()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "sc = StandardScaler()\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "\n",
    "nbr_rep = 6\n",
    "nbr_gen = 2\n",
    "nbr_sol = 2\n",
    "max_eval = nbr_gen*nbr_sol\n",
    "\n",
    "with_smote = False \n",
    "hybrid_option = False # means smote and threshold moving\n",
    "\n",
    "if hybrid_option:\n",
    "    with_smote =True\n",
    "\n",
    "\n",
    "def getDataset(file_name):\n",
    "    dataset = pd.read_csv(\"dataset/\"+file_name, \n",
    "                          parse_dates=['gh_build_started_at'], \n",
    "                          index_col=\"gh_build_started_at\")\n",
    "    dataset.sort_values(by=['gh_build_started_at'], inplace=True)\n",
    "    return dataset\n",
    "    \n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "def getBestThreshold(probs, y_train):\n",
    "    # keep probabilities for the positive outcome only\n",
    "    #probs = predicted_builds[:, 1]\n",
    "    thresholds = arange(0, 1, 0.001)\n",
    "    # evaluate each threshold\n",
    "    scores = [roc_auc_score(y_train, to_labels(probs, t)) for t in thresholds]\n",
    "    # get best threshold\n",
    "    ix = argmax(scores)\n",
    "    #print('\\nThreshold=%.2f, AUC=%.2f' % (thresholds[ix], scores[ix]))\n",
    "    return  thresholds[ix]\n",
    "\n",
    "\n",
    "def failureInfo(dataset):\n",
    "    condition =  dataset['build_Failed'] > 0\n",
    "    rate = (dataset[condition].shape[0]) /dataset.shape[0]\n",
    "    size=dataset.shape[0]\n",
    "    return rate,size\n",
    "\n",
    "def getEntry(y, predicted_builds):\n",
    "    entry = {}\n",
    "    entry[\"AUC\"] =  roc_auc_score(y, predicted_builds)\n",
    "    entry[\"accuracy\"] =  accuracy_score(y, predicted_builds)\n",
    "    entry[\"F1\"] =  f1_score(y,predicted_builds)\n",
    "    return entry\n",
    "\n",
    "def predict_lstm(classifier,X,y):\n",
    "    predicted_builds = classifier.predict(X)\n",
    "    \n",
    "    if with_smote and not hybrid_option:\n",
    "        decision_threshold = 0.5\n",
    "    else:\n",
    "        decision_threshold = getBestThreshold(predicted_builds, y)\n",
    "        \n",
    "    predicted_builds = (predicted_builds >= decision_threshold)\n",
    "    return getEntry(y, predicted_builds)\n",
    "\n",
    "def isInt(n):\n",
    "    try:\n",
    "        n=int(n)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "def online_validation_folds(dataset):\n",
    "    train_sets=[]\n",
    "    test_sets =[]\n",
    "    fold_size = int(len(dataset) * 0.1)\n",
    "    for i in range(6,11):\n",
    "        train_sets.append(dataset.iloc[0:(fold_size*(i-1))])\n",
    "        test_sets.append(dataset.iloc[fold_size*(i-1):(fold_size*i)])\n",
    "    return  train_sets, test_sets\n",
    "def frange(start, stop=None, step=None):\n",
    "\n",
    "    if stop == None:\n",
    "        stop = start + 0.0\n",
    "        start = 0.0\n",
    "\n",
    "    if step == None:\n",
    "        step = 1.0\n",
    "\n",
    "    while True:\n",
    "        if step > 0 and start >= stop:\n",
    "            break\n",
    "        elif step < 0 and start <= stop:\n",
    "            break\n",
    "        yield (\"%g\" % start) # return float number\n",
    "        start = start + step\n",
    "        \n",
    "def frange_int(start, stop=None, step=None):\n",
    "\n",
    "    if stop == None:\n",
    "        stop = start \n",
    "        start = 0\n",
    "\n",
    "    if step == None:\n",
    "        step = 1\n",
    "\n",
    "    while True:\n",
    "        if step > 0 and start >= stop:\n",
    "            break\n",
    "        elif step < 0 and start <= stop:\n",
    "            break\n",
    "        yield (start) # return int number\n",
    "        start = start + step\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 13:22:50.468807: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-16 13:22:50.468945: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp,Trials,STATUS_OK ,fmin,tpe,rand\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import ConfigSpace as CS\n",
    "from hpbandster.core.worker import Worker\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "def train_preprocess(dataset_train,time_step):\n",
    "    training_set = dataset_train.iloc[:,0:19].values\n",
    "    if with_smote:\n",
    "        X= training_set\n",
    "        y= dataset_train.iloc[:,0].values\n",
    "        X, y = SMOTE().fit_resample(X, y)\n",
    "        training_set = X\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(time_step, len(training_set)):\n",
    "        X_train.append(training_set[i-time_step:i, 0])#0 : we have only one column in training_set\n",
    "        y_train.append(training_set[i, 0])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    # Reshaping\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "     #X_train.shape[0] : nbr of lines or observations; X_train.shape[1]:nbr of columns or timestep; 1: nbr of indicators\n",
    "    return X_train,y_train\n",
    "\n",
    "def test_preprocess(dataset_train,dataset_test,time_step):\n",
    "    #Test preprocessing\n",
    "    y_test = dataset_test.iloc[:,0:1].values\n",
    "    dataset_total = pd.concat((dataset_train['build_Failed'], dataset_test['build_Failed']), axis = 0)\n",
    "    inputs = dataset_total[len(dataset_total) - len(dataset_test) - time_step:].values\n",
    "    inputs = inputs.reshape(-1,1)\n",
    "    X_test = []\n",
    "    for j in range(time_step, len(inputs)):\n",
    "        X_test.append(inputs[j-time_step:j, 0])\n",
    "    X_test = np.array(X_test)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    return X_test,y_test\n",
    "\n",
    "def get_threshold_list(dataset):\n",
    "    cdt =  dataset['build_Failed'] > 0\n",
    "    failure_rate = (dataset[cdt].shape[0] /dataset.shape[0])\n",
    "    return list(frange(0.01,max(1,failure_rate), 0.1))\n",
    "\n",
    "class LSTMWorker(Worker):\n",
    "    def __init__(self,  train_set, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_set= train_set\n",
    "\n",
    "    def compute(self, config, *args, **kwargs):\n",
    "        res = construct_lstm_model(config,self.train_set)\n",
    "        return({\n",
    "                    'loss': float(res[\"validation_loss\"]),  # this is the a mandatory field to run hyperband,   \n",
    "                    #remember: HpBandSter always minimizes!\n",
    "                    'info': res[\"entry\"] # can be used for any user-defined information - also mandatory\n",
    "                })\n",
    "\n",
    "def construct_lstm_model (network_params,train_set):\n",
    "    X_train,y_train = train_preprocess(train_set,network_params[\"time_step\"])# need to preprocess each time to tune the time_step\n",
    "    drop = round(network_params[\"drop_proba\"])\n",
    "    # Initialising the RNN\n",
    "    classifier = Sequential()\n",
    "    # Adding the first LSTM layer and some Dropout regularisation\n",
    "    classifier.add(LSTM(units = network_params[\"nb_units\"], return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "    classifier.add(Dropout(drop))\n",
    "    # Adding LSTM layer and some Dropout regularisation\n",
    "    for nbLayesr in range (0,network_params[\"nb_layers\"]):\n",
    "        classifier.add(LSTM(units = network_params[\"nb_units\"], return_sequences = True))\n",
    "        classifier.add(Dropout(drop))\n",
    "    # Adding another LSTM layer without return_sequences\n",
    "    classifier.add(LSTM(units = network_params[\"nb_units\"]))\n",
    "    classifier.add(Dropout(drop))\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 1,activation='sigmoid'))\n",
    "    # Compiling the RNN\n",
    "    classifier.compile(optimizer = network_params[\"optimizer\"],\n",
    "                       loss = 'binary_crossentropy',metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = EarlyStopping(monitor='loss',mode='min', verbose=1,patience=10)\n",
    "    \n",
    "     # Fitting the RNN to the Training set\n",
    "    result =  classifier.fit(X_train, y_train, epochs = network_params[\"nb_epochs\"]\n",
    "                   , batch_size = network_params[\"nb_batch\"],\n",
    "                   verbose=0, callbacks=[es])\n",
    "    \n",
    "    # Get the lowest validation loss of the training epochs\n",
    "    validation_loss = np.amin(result.history['loss']) \n",
    "    # Get prediction probs\n",
    "    entry = predict_lstm(classifier,X_train,y_train)\n",
    "    entry['validation_loss']=validation_loss\n",
    "    return      {\n",
    "                'validation_loss'  : validation_loss, #required by TPE,GA\n",
    "                'model'   : classifier#required by GA\n",
    "                ,\"entry\"  : entry #required by GA\n",
    "                }\n",
    "global data\n",
    "global global_params\n",
    "global global_model\n",
    "global global_entry\n",
    "\n",
    "\n",
    "def evaluate_tuner(tuner_option, train_set):\n",
    "    global data\n",
    "    data = train_set\n",
    "    #########################################\n",
    "    nb_units =  list(frange_int(32,64, 32))#[64]#,128,256\n",
    "    nb_epochs = [4,5,6]#list(frange_int(5,10, 1))#list(frange_int(5,25, 5))#15,20,25,,10\n",
    "    nb_batch =[4,8,16,32, 64]#,, . power of 2\n",
    "    nb_layers = [1,2,3,4]\n",
    "    optimizers = [ 'adam','rmsprop']#,\n",
    "    time_steps = list(frange_int(30,61, 1))\n",
    "    drops = list(frange_int(0.01,0.21, 0.01))\n",
    "    ##########################################################\n",
    "    start = timer()\n",
    "    \n",
    "    rnn_param_choices = {\n",
    "        'nb_units':   nb_units,\n",
    "        'nb_layers':  nb_layers,\n",
    "        'optimizer':  optimizers,\n",
    "        'time_step':  time_steps,\n",
    "        'nb_epochs':  nb_epochs,\n",
    "        'nb_batch':   nb_batch,\n",
    "        'drop_proba': drops\n",
    "        # 'decision_threshold'       :  threshold_list\n",
    "    }\n",
    "    best_params ,best_model , entry_train = generate(rnn_param_choices, construct_lstm_model, data)\n",
    "\n",
    "\n",
    "    end = timer()\n",
    "    period = (end - start)\n",
    "    entry_train[\"time\"] = period\n",
    "    entry_train[\"params\"] = best_params\n",
    "    entry_train[\"model\"]  = best_model\n",
    "    return entry_train\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params of GA {'population_size': 2, 'max_generations': 2, 'retain': 0.7, 'random_select': 0.1, 'mutate_chance': 0.1}\n",
      "*********************************** REP(GA)  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]2022-04-16 13:24:40.503494: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-04-16 13:24:40.503538: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Arch): /proc/driver/nvidia/version does not exist\n",
      "2022-04-16 13:24:40.537591: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [07:31<00:00, 225.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  2  trained\n",
      "Generation average: 77.05%\n",
      "Generation evolving..\n",
      "*********************************** REP(GA)  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 491.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 77.32%\n",
      "Generation evolving..\n",
      "for params  {'nb_units': 32, 'nb_layers': 1, 'optimizer': 'rmsprop', 'time_step': 56, 'nb_epochs': 6, 'nb_batch': 64, 'drop_proba': 0.05} the score in the train =  0.7732175171914585\n",
      "1 *************************************** TRAIN jruby.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_train {'AUC': 0.7473847866423654, 'accuracy': 0.7395460969324134, 'F1': 0.7732175171914585, 'validation_loss': 0.5090203285217285, 'time': 451.70104115499987, 'params': {'nb_units': 32, 'nb_layers': 1, 'optimizer': 'rmsprop', 'time_step': 56, 'nb_epochs': 6, 'nb_batch': 64, 'drop_proba': 0.05}, 'model': <keras.engine.sequential.Sequential object at 0x7f5dbdba1630>, 'iter': 1, 'proj': 'jruby.csv', 'algo': 'LSTM'}\n",
      "params of GA {'population_size': 2, 'max_generations': 2, 'retain': 0.7, 'random_select': 0.1, 'mutate_chance': 0.1}\n",
      "*********************************** REP(GA)  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [25:36<00:00, 768.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  2  trained\n",
      "Generation average: 77.51%\n",
      "Generation evolving..\n",
      "*********************************** REP(GA)  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 782.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 77.89%\n",
      "Generation evolving..\n",
      "for params  {'nb_units': 32, 'nb_layers': 4, 'optimizer': 'adam', 'time_step': 39, 'nb_epochs': 6, 'nb_batch': 4, 'drop_proba': 0.05} the score in the train =  0.7788641265276779\n",
      "2 *************************************** TRAIN jruby.csv\n",
      "entry_train {'AUC': 0.7509139469624725, 'accuracy': 0.7446455254856383, 'F1': 0.7788641265276779, 'validation_loss': 0.5080700516700745, 'time': 1536.380909673, 'params': {'nb_units': 32, 'nb_layers': 4, 'optimizer': 'adam', 'time_step': 39, 'nb_epochs': 6, 'nb_batch': 4, 'drop_proba': 0.05}, 'model': <keras.engine.sequential.Sequential object at 0x7f5d75cfab60>, 'iter': 2, 'proj': 'jruby.csv', 'algo': 'LSTM'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params of GA {'population_size': 2, 'max_generations': 2, 'retain': 0.7, 'random_select': 0.1, 'mutate_chance': 0.1}\n",
      "*********************************** REP(GA)  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  2  trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [16:10<00:00, 485.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 77.52%\n",
      "Generation evolving..\n",
      "*********************************** REP(GA)  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 396.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 78.22%\n",
      "Generation evolving..\n",
      "for params  {'nb_units': 32, 'nb_layers': 3, 'optimizer': 'adam', 'time_step': 31, 'nb_epochs': 4, 'nb_batch': 4, 'drop_proba': 0.15} the score in the train =  0.7822000711997151\n",
      "3 *************************************** TRAIN jruby.csv\n",
      "entry_train {'AUC': 0.7498226526362535, 'accuracy': 0.7462253193960511, 'F1': 0.7822000711997151, 'validation_loss': 0.5096526741981506, 'time': 970.0381974339998, 'params': {'nb_units': 32, 'nb_layers': 3, 'optimizer': 'adam', 'time_step': 31, 'nb_epochs': 4, 'nb_batch': 4, 'drop_proba': 0.15}, 'model': <keras.engine.sequential.Sequential object at 0x7f5d75eee590>, 'iter': 3, 'proj': 'jruby.csv', 'algo': 'LSTM'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params of GA {'population_size': 2, 'max_generations': 2, 'retain': 0.7, 'random_select': 0.1, 'mutate_chance': 0.1}\n",
      "*********************************** REP(GA)  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "solution  2  trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [09:14<00:00, 277.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation average: 78.19%\n",
      "Generation evolving..\n",
      "*********************************** REP(GA)  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 105.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 78.81%\n",
      "Generation evolving..\n",
      "for params  {'nb_units': 32, 'nb_layers': 4, 'optimizer': 'adam', 'time_step': 59, 'nb_epochs': 5, 'nb_batch': 32, 'drop_proba': 0.12999999999999998} the score in the train =  0.7880576044959607\n",
      "4 *************************************** TRAIN jruby.csv\n",
      "entry_train {'AUC': 0.7485694019860536, 'accuracy': 0.7491268917345751, 'F1': 0.7880576044959607, 'validation_loss': 0.5109725594520569, 'time': 554.5104569660002, 'params': {'nb_units': 32, 'nb_layers': 4, 'optimizer': 'adam', 'time_step': 59, 'nb_epochs': 5, 'nb_batch': 32, 'drop_proba': 0.12999999999999998}, 'model': <keras.engine.sequential.Sequential object at 0x7f5d3c700820>, 'iter': 4, 'proj': 'jruby.csv', 'algo': 'LSTM'}\n",
      "params of GA {'population_size': 2, 'max_generations': 2, 'retain': 0.7, 'random_select': 0.1, 'mutate_chance': 0.1}\n",
      "*********************************** REP(GA)  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  2  trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [04:48<00:00, 144.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 76.96%\n",
      "Generation evolving..\n",
      "*********************************** REP(GA)  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 225.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 77.42%\n",
      "Generation evolving..\n",
      "for params  {'nb_units': 32, 'nb_layers': 1, 'optimizer': 'rmsprop', 'time_step': 37, 'nb_epochs': 5, 'nb_batch': 32, 'drop_proba': 0.09999999999999999} the score in the train =  0.7742215785662564\n",
      "5 *************************************** TRAIN jruby.csv\n",
      "entry_train {'AUC': 0.7494572695485425, 'accuracy': 0.7412018592297477, 'F1': 0.7742215785662564, 'validation_loss': 0.5108311772346497, 'time': 288.5257944089999, 'params': {'nb_units': 32, 'nb_layers': 1, 'optimizer': 'rmsprop', 'time_step': 37, 'nb_epochs': 5, 'nb_batch': 32, 'drop_proba': 0.09999999999999999}, 'model': <keras.engine.sequential.Sequential object at 0x7f5d21fe68c0>, 'iter': 5, 'proj': 'jruby.csv', 'algo': 'LSTM'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "global columns_res,columns_comp\n",
    "columns_res = [\"proj\"]+[\"algo\"]+[\"iter\"]+[\"AUC\"]+[\"accuracy\"]+[\"F1\"]+[\"exp\"]\n",
    "\n",
    "tuner = \"ga\"\n",
    "results = pd.DataFrame(columns =  columns_res)\n",
    "results_train = pd.DataFrame(columns =  columns_res)\n",
    "bellwether=\"jruby.csv\"\n",
    "trainset = getDataset(bellwether)\n",
    "for iteration in range (1,nbr_rep):\n",
    "    entry_train  = evaluate_tuner(tuner,trainset)\n",
    "    best_params = entry_train[\"params\"]\n",
    "    best_model = entry_train[\"model\"]\n",
    "    print(iteration,\"*************************************** TRAIN\",bellwether)\n",
    "    entry_train[\"iter\"] = iteration\n",
    "    entry_train[\"proj\"] = bellwether\n",
    "    entry_train[\"algo\"] = \"LSTM\"\n",
    "    entry_train[\"params\"] = best_params\n",
    "    results_train = results_train.append(entry_train,ignore_index=True)\n",
    "    print(\"entry_train\",entry_train)\n",
    "    for file_name in os.listdir(\"dataset\"):\n",
    "        if file_name!=bellwether:\n",
    "            #print(file_name)\n",
    "            testset = getDataset(file_name)\n",
    "            X,y = test_preprocess(trainset,testset,best_params[\"time_step\"])\n",
    "            entry= predict_lstm(best_model,X,y)\n",
    "            entry[\"iter\"] = iteration\n",
    "            entry[\"proj\"] = file_name\n",
    "            entry[\"exp\"] =  1\n",
    "            entry[\"algo\"] = \"LSTM\"\n",
    "            results = results.append(entry,ignore_index=True)\n",
    "results.to_excel(\"corss_proj_paramf_\"+str(hybrid_option)+str(with_smote)+\"_result_crossProj_\"+tuner+\"_LSTM.xlsx\")\n",
    "results_train.to_excel(\"cross_paramf\"+str(hybrid_option)+str(with_smote)+\"_train_crossProj_\"+tuner+\"_LSTM.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
