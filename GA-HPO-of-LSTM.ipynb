{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optunity \n",
    "# !pip install ConfigSpace\n",
    "# !pip install hpbandster \n",
    "# !pip install hyperopt \n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class that represents the solution to be evolved.\"\"\"\n",
    "import random\n",
    "class Solution():\n",
    "    def __init__(self, all_possible_params):\n",
    "        self.entry = {}\n",
    "        self.score = 0.\n",
    "        self.all_possible_params = all_possible_params\n",
    "        self.params = {}  #  represents model parameters to be picked by creat_random method\n",
    "        self.model = None\n",
    "        \n",
    "    \"\"\"Create the model random params.\"\"\"\n",
    "    def create_random(self):\n",
    "        for key in self.all_possible_params:\n",
    "            self.params[key] = random.choice(self.all_possible_params[key])\n",
    "\n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "      \n",
    "    \"\"\"\n",
    "        Train the model and record the score.\n",
    "    \"\"\"\n",
    "    def train_model(self, fn_train,params_fn):\n",
    "        \n",
    "        if self.score == 0.:\n",
    "                res = fn_train(self.params,params_fn)\n",
    "                self.score =  res[\"entry\"][\"F1\"] #1-float(res[\"validation_loss\"])\n",
    "                self.model = res[\"model\"]\n",
    "                self.entry = res['entry']\n",
    "            \n",
    "    \"\"\"Print out a network.\"\"\"\n",
    "    def print_solution(self):\n",
    "        print(\"for params \", self.params , \"the score in the train = \",self.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class that holds a genetic algorithm for evolving a population of params.\n",
    "\"\"\"\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import random\n",
    "\"\"\"Class that implements genetic algorithm for Hyper-parameter tuning\"\"\"\n",
    "class Optimizer():\n",
    "    \n",
    "    def __init__(self, GA_params, all_possible_params):\n",
    "        \"\"\"Create an optimizer.\"\"\"\n",
    "        self.random_select = GA_params[\"random_select\"]\n",
    "        self.mutate_chance = GA_params[\"mutate_chance\"]\n",
    "        self.retain = GA_params[\"retain\"]\n",
    "        self.all_possible_params = all_possible_params\n",
    "    \n",
    "    def create_population(self, count):\n",
    "        \"\"\"Create a population of random solutions.\"\"\"\n",
    "        pop = []\n",
    "        for _ in range(0, count):\n",
    "            # Create a random solution.\n",
    "            solution = Solution(self.all_possible_params)\n",
    "            solution.create_random()\n",
    "            # Add the solution to our population.\n",
    "            pop.append(solution)\n",
    "        return pop\n",
    "\n",
    "    @staticmethod\n",
    "    def fitness(solution):\n",
    "        \"\"\"Return the score, which is our fitness function.\"\"\"\n",
    "        return solution.score\n",
    "\n",
    "    def grade(self, pop):\n",
    "        \"\"\"Find average fitness for a population. \"\"\"\n",
    "        summed = reduce(add, (self.fitness(solution) for solution in pop))\n",
    "        return summed / float((len(pop)))\n",
    "\n",
    "    def crossover(self, mother, father):\n",
    "        \"\"\"Make two children as parts of their parents.\n",
    "        Args:\n",
    "            mother (dict): parameters\n",
    "            father (dict): parameters\n",
    "        Returns:\n",
    "            (list): combined params\n",
    "        \"\"\"\n",
    "        children = []\n",
    "        for _ in range(2):\n",
    "            child = {}\n",
    "            # Loop through the parameters and pick params for the kid.\n",
    "            for param in self.all_possible_params:\n",
    "                child[param] = random.choice([mother.params[param], father.params[param]] )\n",
    "\n",
    "            solution = Solution(self.all_possible_params)\n",
    "            solution.set_params(child)\n",
    "            # Randomly mutate some of the children.\n",
    "            if self.mutate_chance > random.random():\n",
    "                solution = self.mutate(solution)\n",
    "            children.append(solution)\n",
    "        return children\n",
    "    \n",
    "    \n",
    "    def mutate(self, solution):\n",
    "        \"\"\"Randomly mutate one part of the solution.\"\"\"\n",
    "        # Choose a random key.\n",
    "        mutation = random.choice(list(self.all_possible_params.keys()))\n",
    "        # Mutate one of the params.\n",
    "        solution.params[mutation] = random.choice(self.all_possible_params[mutation])\n",
    "        return solution\n",
    "    \n",
    "    \"\"\"Evolve a population of solutions.\"\"\"\n",
    "    def evolve(self, pop):\n",
    "        #Get scores for each solution.\n",
    "        graded = [(self.fitness(solution), solution) for solution in pop]\n",
    "        #\"Sort on the scores.\n",
    "        graded = [x[1] for x in sorted(graded, key=lambda x: x[0], reverse=True)]\n",
    "        #Get the number we want to keep for the next gen.\n",
    "        retain_length = int(len(graded)*self.retain)\n",
    "        # define what we want to keep.\n",
    "        parents = graded[:retain_length]\n",
    "        # For those we aren't keeping, randomly keep some anyway.\n",
    "        for individual in graded[retain_length:]:\n",
    "            if self.random_select > random.random():\n",
    "                parents.append(individual)\n",
    "        # Now find out how many spots we have left to fill.\n",
    "        parents_length = len(parents)\n",
    "        desired_length = len(pop) - parents_length\n",
    "        \n",
    "        # Add children, which are bred from two remaining solutions.\n",
    "        if parents_length > 1 and desired_length> 0:\n",
    "            children = []\n",
    "            while len(children) < desired_length:\n",
    "                if parents_length==2:\n",
    "                    male_index = 1\n",
    "                    female_index = 0\n",
    "                else:\n",
    "                    male_index = random.randint(0, parents_length-1)\n",
    "                    female_index = random.randint(0, parents_length-1)\n",
    "                \n",
    "                # Assuming they aren't the same solutions...\n",
    "                if male_index != female_index:\n",
    "                    print(\"Get a random mom and dad.\")\n",
    "                    male = parents[male_index]\n",
    "                    female = parents[female_index]\n",
    "                    # crossover them.\n",
    "                    babies = self.crossover(male, female)\n",
    "                    # Add the children one at a time.\n",
    "                    for baby in babies:\n",
    "                        # Don't grow larger than desired length.\n",
    "                        if len(children) < desired_length:\n",
    "                            children.append(baby)\n",
    "            parents.extend(children)\n",
    "        return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import threading\n",
    "def train_sol_thread(solution,fn_train,params_fn,i):\n",
    "    solution.train_model(fn_train,params_fn)\n",
    "    print(\"solution \", i,\" trained\")\n",
    "    \n",
    "def train_population(pop, fn_train,params_fn):\n",
    "    pbar = tqdm(total=len(pop))\n",
    "    threads = list()\n",
    "    i=1\n",
    "    for solution in pop:\n",
    "        x = threading.Thread(target=train_sol_thread, args=(solution,fn_train,params_fn,i))\n",
    "        i=i+1\n",
    "        threads.append(x)\n",
    "        x.start()\n",
    "        pbar.update(1)\n",
    "        \n",
    "    for index, thread in enumerate(threads):\n",
    "        thread.join()\n",
    "    pbar.close()\n",
    "\n",
    "\n",
    "def get_average_score(pop):\n",
    "    \"\"\"Get the average score for a group of solutions.\"\"\"\n",
    "    total_scores = 0\n",
    "    for solution in pop:\n",
    "        total_scores += solution.score\n",
    "    return total_scores / len(pop)\n",
    "\n",
    "\n",
    "def generate(all_possible_params, fn_train , params_fn):\n",
    "    \"\"\"Generate the optimal params with the genetic algorithm.\"\"\"\n",
    "    \"\"\" Args:\n",
    "            GA_params: Params for GA\n",
    "            all_possible_params (dict): Parameter choices for the model\n",
    "            train_set : training dataset\n",
    "            fn_train : a function used to compute the prediction accuracy\n",
    "    \"\"\"\n",
    "   \n",
    "    GA_params = {\n",
    "            \"population_size\": nbr_sol,\n",
    "            \"max_generations\": nbr_gen,\n",
    "            \"retain\": 0.7,\n",
    "            \"random_select\":0.1,\n",
    "            \"mutate_chance\":0.1\n",
    "            }\n",
    "    \n",
    "    print(\"params of GA\" , GA_params)\n",
    "    optimizer = Optimizer(GA_params ,all_possible_params)\n",
    "    pop = optimizer.create_population(GA_params['population_size'])\n",
    "    # Evolve the generation.\n",
    "    for i in range(GA_params['max_generations']):\n",
    "        print(\"*********************************** REP(GA) \",(i+1))\n",
    "        # Train and get accuracy for solutions.\n",
    "        train_population(pop,fn_train,params_fn)\n",
    "        # Get the average accuracy for this generation.\n",
    "        average_accuracy = get_average_score(pop)\n",
    "        # Print out the average accuracy each generation.\n",
    "        print(\"Generation average: %.2f%%\" % (average_accuracy * 100))\n",
    "        # Evolve, except on the last iteration.\n",
    "        if i != (GA_params['max_generations']):\n",
    "            print(\"Generation evolving..\")\n",
    "            evolved = optimizer.evolve(pop)\n",
    "            if(len(evolved)!=0):\n",
    "                pop=evolved\n",
    "        else:\n",
    "            pop = sorted(pop, key=lambda x: x.score, reverse=True)\n",
    "    # Print out the top 2 solutions.\n",
    "    size = len(pop)\n",
    "    if size < 3:\n",
    "        print_pop(pop[:size])\n",
    "    else:\n",
    "        print_pop(pop[:3])\n",
    "    return pop[0].params ,pop[0].model,pop[0].entry\n",
    "\n",
    "def print_pop(pop):\n",
    "    for solution in pop:\n",
    "        solution.print_solution()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "sc = StandardScaler()\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "\n",
    "nbr_rep = 1 # repreats?\n",
    "nbr_gen = 10\n",
    "nbr_sol = 6\n",
    "max_eval = nbr_gen*nbr_sol\n",
    "\n",
    "with_smote = False \n",
    "hybrid_option = False # means smote and threshold moving\n",
    "\n",
    "if hybrid_option:\n",
    "    with_smote =True\n",
    "\n",
    "import os\n",
    "\n",
    "def getDataset(file_name):\n",
    "    dataset = pd.read_csv(\"ordered-data/\"+file_name, \n",
    "                          #parse_dates=['date'], \n",
    "                          index_col=\"date\")\n",
    "    dataset.sort_values(by=['date'], inplace=True)\n",
    "    return dataset\n",
    "\n",
    "def getDataset_2(valid_proj, type):\n",
    "    # dataset=getDataset(valid_proj)\n",
    "    # #print(dataset)\n",
    "    # return dataset\n",
    "    columns = ['ci_skipped', 'ns', 'nd', 'nf', 'entropy', 'la', 'ld', 'lt', 'ndev',\n",
    "        'age', 'nuc', 'exp', 'rexp', 'sexp', 'TFC', 'is_doc', 'is_build',\n",
    "        'is_meta', 'is_media', 'is_src', 'is_merge', 'FRM', 'COM', 'CFT',\n",
    "        'classif', 'prev_com_res', 'proj_recent_skip', 'comm_recent_skip',\n",
    "        'same_committer', 'is_fix', 'day_week', 'CM', 'commit_hash']\n",
    "    cols_to_keep = 32\n",
    "\n",
    "    if type==\"train\":\n",
    "        df_train = pd.DataFrame(columns=columns, dtype=np.float)\n",
    "        for dirname, _, filenames in os.walk(\"ordered-data\"):\n",
    "            for filename in filenames:\n",
    "                if filename[-4:]==\".csv\" and filename!=valid_proj:\n",
    "                    new_data = pd.read_csv(os.path.join(dirname, filename),index_col=\"date\")\n",
    "                    new_data.sort_values(by=['date'], inplace=True)\n",
    "                    df_train = pd.concat([df_train, new_data])\n",
    "        # print(df_train.shape) # (15001, 34) => (422, 33)\n",
    "        # print(df_train)\n",
    "        return df_train\n",
    "\n",
    "    if type==\"test\":\n",
    "        df_test = pd.read_csv(os.path.join(\"ordered-data\", valid_proj),index_col=\"date\")\n",
    "        df_test.sort_values(by=['date'], inplace=True)\n",
    "        return df_test\n",
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "def getBestThreshold(probs, y_train):\n",
    "    # keep probabilities for the positive outcome only\n",
    "    #probs = predicted_builds[:, 1]\n",
    "    thresholds = arange(0, 1, 0.001)\n",
    "    # evaluate each threshold\n",
    "    scores = [roc_auc_score(y_train, to_labels(probs, t)) for t in thresholds]\n",
    "    # get best threshold\n",
    "    ix = argmax(scores)\n",
    "    #print('\\nThreshold=%.2f, AUC=%.2f' % (thresholds[ix], scores[ix]))\n",
    "    return  thresholds[ix]\n",
    "\n",
    "\n",
    "def failureInfo(dataset):\n",
    "    condition =  dataset['ci_skipped'] > 0\n",
    "    rate = (dataset[condition].shape[0]) /dataset.shape[0]\n",
    "    size=dataset.shape[0]\n",
    "    return rate,size\n",
    "\n",
    "def getEntry(y, predicted_builds):\n",
    "    entry = {}\n",
    "    entry[\"AUC\"] =  roc_auc_score(y, predicted_builds)\n",
    "    entry[\"accuracy\"] =  accuracy_score(y, predicted_builds)\n",
    "    entry[\"F1\"] =  f1_score(y,predicted_builds)\n",
    "    return entry\n",
    "\n",
    "def predict_lstm(classifier,X,y):\n",
    "    predicted_builds = classifier.predict(X)\n",
    "    \n",
    "    if with_smote and not hybrid_option:\n",
    "        decision_threshold = 0.5\n",
    "    else:\n",
    "        decision_threshold = getBestThreshold(predicted_builds, y)\n",
    "        \n",
    "    predicted_builds = (predicted_builds >= decision_threshold)\n",
    "    return getEntry(y, predicted_builds)\n",
    "\n",
    "def isInt(n):\n",
    "    try:\n",
    "        n=int(n)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "def online_validation_folds(dataset):\n",
    "    train_sets=[]\n",
    "    test_sets =[]\n",
    "    fold_size = int(len(dataset) * 0.1)\n",
    "    for i in range(6,11):\n",
    "        train_sets.append(dataset.iloc[0:(fold_size*(i-1))])\n",
    "        test_sets.append(dataset.iloc[fold_size*(i-1):(fold_size*i)])\n",
    "    return  train_sets, test_sets\n",
    "def frange(start, stop=None, step=None):\n",
    "\n",
    "    if stop == None:\n",
    "        stop = start + 0.0\n",
    "        start = 0.0\n",
    "\n",
    "    if step == None:\n",
    "        step = 1.0\n",
    "\n",
    "    while True:\n",
    "        if step > 0 and start >= stop:\n",
    "            break\n",
    "        elif step < 0 and start <= stop:\n",
    "            break\n",
    "        yield (\"%g\" % start) # return float number\n",
    "        start = start + step\n",
    "        \n",
    "def frange_int(start, stop=None, step=None):\n",
    "\n",
    "    if stop == None:\n",
    "        stop = start \n",
    "        start = 0\n",
    "\n",
    "    if step == None:\n",
    "        step = 1\n",
    "\n",
    "    while True:\n",
    "        if step > 0 and start >= stop:\n",
    "            break\n",
    "        elif step < 0 and start <= stop:\n",
    "            break\n",
    "        yield (start) # return int number\n",
    "        start = start + step\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 12:53:15.635618: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-16 12:53:15.635657: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp,Trials,STATUS_OK ,fmin,tpe,rand\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import ConfigSpace as CS\n",
    "from hpbandster.core.worker import Worker\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "def train_preprocess(dataset_train,time_step):\n",
    "    training_set = dataset_train.iloc[:,0:32].values\n",
    "    if with_smote:\n",
    "        X= training_set\n",
    "        y= dataset_train.iloc[:,0].values\n",
    "        X, y = SMOTE().fit_resample(X, y)\n",
    "        training_set = X\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(time_step, len(training_set)):\n",
    "        X_train.append(training_set[i-time_step:i, 0])#0 : we have only one column in training_set\n",
    "        y_train.append(training_set[i, 0])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    # Reshaping\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    # X_train.shape[0] : nbr of lines or observations; X_train.shape[1]:nbr of columns or timestep; 1: nbr of indicators\n",
    "    return X_train,y_train\n",
    "\n",
    "def test_preprocess(dataset_train,dataset_test,time_step):\n",
    "    #Test preprocessing\n",
    "    y_test = dataset_test.iloc[:,0:1].values\n",
    "    dataset_total = pd.concat((dataset_train['ci_skipped'], dataset_test['ci_skipped']), axis = 0)\n",
    "    inputs = dataset_total[len(dataset_total) - len(dataset_test) - time_step:].values\n",
    "    inputs = inputs.reshape(-1,1)\n",
    "    X_test = []\n",
    "    for j in range(time_step, len(inputs)):\n",
    "        X_test.append(inputs[j-time_step:j, 0])\n",
    "    X_test = np.array(X_test)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    return X_test,y_test\n",
    "\n",
    "def get_threshold_list(dataset):\n",
    "    cdt =  dataset['ci_skipped'] > 0\n",
    "    failure_rate = (dataset[cdt].shape[0] /dataset.shape[0])\n",
    "    return list(frange(0.01,max(1,failure_rate), 0.1))\n",
    "\n",
    "class LSTMWorker(Worker):\n",
    "    def __init__(self,  train_set, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_set= train_set\n",
    "\n",
    "    def compute(self, config, *args, **kwargs):\n",
    "        res = construct_lstm_model(config,self.train_set)\n",
    "        return({\n",
    "                    'loss': float(res[\"validation_loss\"]),  # this is the a mandatory field to run hyperband,   \n",
    "                    #remember: HpBandSter always minimizes!\n",
    "                    'info': res[\"entry\"] # can be used for any user-defined information - also mandatory\n",
    "                })\n",
    "\n",
    "def construct_lstm_model (network_params,train_set):\n",
    "    X_train,y_train = train_preprocess(train_set,network_params[\"time_step\"])# need to preprocess each time to tune the time_step\n",
    "    drop = round(network_params[\"drop_proba\"])\n",
    "    # Initialising the RNN\n",
    "    classifier = Sequential()\n",
    "    # Adding the first LSTM layer and some Dropout regularisation\n",
    "    classifier.add(LSTM(units = network_params[\"nb_units\"], return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "    classifier.add(Dropout(drop))\n",
    "    # Adding LSTM layer and some Dropout regularisation\n",
    "    for nbLayesr in range (0,network_params[\"nb_layers\"]):\n",
    "        classifier.add(LSTM(units = network_params[\"nb_units\"], return_sequences = True))\n",
    "        classifier.add(Dropout(drop))\n",
    "    # Adding another LSTM layer without return_sequences\n",
    "    classifier.add(LSTM(units = network_params[\"nb_units\"]))\n",
    "    classifier.add(Dropout(drop))\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 1,activation='sigmoid'))\n",
    "    # Compiling the RNN\n",
    "    classifier.compile(optimizer = network_params[\"optimizer\"],\n",
    "                       loss = 'binary_crossentropy',metrics=[\"accuracy\"])\n",
    "    \n",
    "    es = EarlyStopping(monitor='loss',mode='min', verbose=1,patience=10)\n",
    "    \n",
    "     # Fitting the RNN to the Training set\n",
    "    result =  classifier.fit(X_train, y_train, epochs = network_params[\"nb_epochs\"]\n",
    "                   , batch_size = network_params[\"nb_batch\"],\n",
    "                   verbose=0, callbacks=[es])\n",
    "    \n",
    "    # Get the lowest validation loss of the training epochs\n",
    "    validation_loss = np.amin(result.history['loss']) \n",
    "    # Get prediction probs\n",
    "    entry = predict_lstm(classifier,X_train,y_train)\n",
    "    entry['validation_loss']=validation_loss\n",
    "    return      {\n",
    "                'validation_loss'  : validation_loss, #required by TPE,GA\n",
    "                'model'   : classifier#required by GA\n",
    "                ,\"entry\"  : entry #required by GA\n",
    "                }\n",
    "global data\n",
    "global global_params\n",
    "global global_model\n",
    "global global_entry\n",
    "\n",
    "\n",
    "def evaluate_tuner(tuner_option, train_set):\n",
    "    global data\n",
    "    data = train_set\n",
    "    #########################################\n",
    "    nb_units =  list(frange_int(32,64, 32))#[64]#,128,256\n",
    "    nb_epochs = [4,5,6]#list(frange_int(5,10, 1))#list(frange_int(5,25, 5))#15,20,25,,10\n",
    "    nb_batch =[4,8,16,32, 64]#,, . power of 2\n",
    "    nb_layers = [1,2,3,4]\n",
    "    optimizers = [ 'adam','rmsprop']#,\n",
    "    time_steps = list(frange_int(30,61, 1))\n",
    "    drops = list(frange_int(0.01,0.21, 0.01))\n",
    "    ##########################################################\n",
    "    start = timer()\n",
    "    \n",
    "    rnn_param_choices = {\n",
    "        'nb_units':   nb_units,\n",
    "        'nb_layers':  nb_layers,\n",
    "        'optimizer':  optimizers,\n",
    "        'time_step':  time_steps,\n",
    "        'nb_epochs':  nb_epochs,\n",
    "        'nb_batch':   nb_batch,\n",
    "        'drop_proba': drops\n",
    "        # 'decision_threshold'       :  threshold_list\n",
    "    }\n",
    "    best_params ,best_model , entry_train = generate(rnn_param_choices, construct_lstm_model, data)\n",
    "\n",
    "\n",
    "    end = timer()\n",
    "    period = (end - start)\n",
    "    entry_train[\"time\"] = period\n",
    "    entry_train[\"params\"] = best_params\n",
    "    entry_train[\"model\"]  = best_model\n",
    "    return entry_train\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params of GA {'population_size': 2, 'max_generations': 2, 'retain': 0.7, 'random_select': 0.1, 'mutate_chance': 0.1}\n",
      "*********************************** REP(GA)  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 34s 62ms/step\n",
      "solution  1  trained\n",
      "468/468 [==============================] - 19s 34ms/step\n",
      "solution  2  trained\n",
      "468/468 [==============================] - 22s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [29:43<00:00, 891.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 50.43%\n",
      "Generation evolving..\n",
      "*********************************** REP(GA)  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 829.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "solution  2  trained\n",
      "Generation average: 50.43%\n",
      "Generation evolving..\n",
      "for params  {'nb_units': 32, 'nb_layers': 3, 'optimizer': 'rmsprop', 'time_step': 55, 'nb_epochs': 4, 'nb_batch': 4, 'drop_proba': 0.16} the score in the train =  0.5118193578736917\n",
      "1 *************************************** TRAIN steve.csv\n",
      "entry_train {'AUC': 0.7204684091546336, 'accuracy': 0.7222668272447478, 'F1': 0.5118193578736917, 'validation_loss': 0.4235297739505768, 'time': 1783.639093826001, 'params': {'nb_units': 32, 'nb_layers': 3, 'optimizer': 'rmsprop', 'time_step': 55, 'nb_epochs': 4, 'nb_batch': 4, 'drop_proba': 0.16}, 'model': <keras.engine.sequential.Sequential object at 0x7f2ff9b72ce0>, 'iter': 1, 'proj': 'steve.csv', 'algo': 'LSTM'}\n",
      "1/9 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 51ms/step\n",
      "11/11 [==============================] - 1s 46ms/step\n",
      "14/14 [==============================] - 1s 42ms/step\n",
      "21/21 [==============================] - 1s 41ms/step\n",
      "13/13 [==============================] - 1s 43ms/step\n",
      "5/5 [==============================] - 0s 39ms/step\n",
      "21/21 [==============================] - 1s 41ms/step\n",
      "47/47 [==============================] - 2s 41ms/step\n",
      "14/14 [==============================] - 1s 43ms/step\n",
      "58/58 [==============================] - 3s 43ms/step\n",
      "249/249 [==============================] - 10s 39ms/step\n",
      "14/14 [==============================] - 1s 42ms/step\n",
      "params of GA {'population_size': 2, 'max_generations': 2, 'retain': 0.7, 'random_select': 0.1, 'mutate_chance': 0.1}\n",
      "*********************************** REP(GA)  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 20s 35ms/step\n",
      "solution  2  trained\n",
      "468/468 [==============================] - 19s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [06:36<00:00, 198.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 50.69%\n",
      "Generation evolving..\n",
      "*********************************** REP(GA)  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1465.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 50.81%\n",
      "Generation evolving..\n",
      "for params  {'nb_units': 32, 'nb_layers': 3, 'optimizer': 'rmsprop', 'time_step': 37, 'nb_epochs': 5, 'nb_batch': 64, 'drop_proba': 0.05} the score in the train =  0.5081106313455479\n",
      "2 *************************************** TRAIN steve.csv\n",
      "entry_train {'AUC': 0.716630083907347, 'accuracy': 0.7183239775461107, 'F1': 0.5081106313455479, 'validation_loss': 0.40816858410835266, 'time': 396.18207293199885, 'params': {'nb_units': 32, 'nb_layers': 3, 'optimizer': 'rmsprop', 'time_step': 37, 'nb_epochs': 5, 'nb_batch': 64, 'drop_proba': 0.05}, 'model': <keras.engine.sequential.Sequential object at 0x7f3077b07d00>, 'iter': 2, 'proj': 'steve.csv', 'algo': 'LSTM'}\n",
      "1/9 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 35ms/step\n",
      "11/11 [==============================] - 0s 34ms/step\n",
      "14/14 [==============================] - 0s 31ms/step\n",
      "21/21 [==============================] - 1s 32ms/step\n",
      "13/13 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "21/21 [==============================] - 1s 32ms/step\n",
      "47/47 [==============================] - 1s 29ms/step\n",
      "14/14 [==============================] - 0s 33ms/step\n",
      "58/58 [==============================] - 2s 30ms/step\n",
      "249/249 [==============================] - 8s 30ms/step\n",
      "14/14 [==============================] - 0s 31ms/step\n",
      "params of GA {'population_size': 2, 'max_generations': 2, 'retain': 0.7, 'random_select': 0.1, 'mutate_chance': 0.1}\n",
      "*********************************** REP(GA)  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 27s 50ms/step\n",
      "467/467 [==============================] - 15s 27ms/step\n",
      "solution  2  trained\n",
      "solution  2  trained\n",
      "468/468 [==============================] - 17s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [31:27<00:00, 943.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 51.01%\n",
      "Generation evolving..\n",
      "*********************************** REP(GA)  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 440.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution  1  trained\n",
      "Generation average: 51.01%\n",
      "Generation evolving..\n",
      "for params  {'nb_units': 32, 'nb_layers': 1, 'optimizer': 'rmsprop', 'time_step': 58, 'nb_epochs': 5, 'nb_batch': 32, 'drop_proba': 0.17} the score in the train =  0.5100929916080743\n",
      "3 *************************************** TRAIN steve.csv\n",
      "entry_train {'AUC': 0.7225868985790546, 'accuracy': 0.710901425416583, 'F1': 0.5100929916080743, 'validation_loss': 0.4039722979068756, 'time': 1887.5135207849999, 'params': {'nb_units': 32, 'nb_layers': 1, 'optimizer': 'rmsprop', 'time_step': 58, 'nb_epochs': 5, 'nb_batch': 32, 'drop_proba': 0.17}, 'model': <keras.engine.sequential.Sequential object at 0x7f2ff39ebcd0>, 'iter': 3, 'proj': 'steve.csv', 'algo': 'LSTM'}\n",
      "4/9 [============>.................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 21ms/step\n",
      "14/14 [==============================] - 0s 21ms/step\n",
      "21/21 [==============================] - 1s 27ms/step\n",
      "13/13 [==============================] - 0s 20ms/step\n",
      "5/5 [==============================] - 0s 22ms/step\n",
      "21/21 [==============================] - 0s 21ms/step\n",
      "47/47 [==============================] - 1s 21ms/step\n",
      "14/14 [==============================] - 0s 22ms/step\n",
      "58/58 [==============================] - 1s 22ms/step\n",
      "249/249 [==============================] - 5s 22ms/step\n",
      "14/14 [==============================] - 0s 22ms/step\n",
      "params of GA {'population_size': 2, 'max_generations': 2, 'retain': 0.7, 'random_select': 0.1, 'mutate_chance': 0.1}\n",
      "*********************************** REP(GA)  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 21s 36ms/step\n",
      "solution  1  trained\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m trainset \u001b[39m=\u001b[39m getDataset_2(bellwether,\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (\u001b[39m1\u001b[39m,nbr_rep):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     entry_train  \u001b[39m=\u001b[39m evaluate_tuner(tuner,trainset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     best_params \u001b[39m=\u001b[39m entry_train[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     best_model \u001b[39m=\u001b[39m entry_train[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;32m/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb Cell 11\u001b[0m in \u001b[0;36mevaluate_tuner\u001b[0;34m(tuner_option, train_set)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m start \u001b[39m=\u001b[39m timer()\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m rnn_param_choices \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnb_units\u001b[39m\u001b[39m'\u001b[39m:   nb_units,\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnb_layers\u001b[39m\u001b[39m'\u001b[39m:  nb_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     \u001b[39m# 'decision_threshold'       :  threshold_list\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m }\n\u001b[0;32m--> <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m best_params ,best_model , entry_train \u001b[39m=\u001b[39m generate(rnn_param_choices, construct_lstm_model, data)\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m end \u001b[39m=\u001b[39m timer()\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m period \u001b[39m=\u001b[39m (end \u001b[39m-\u001b[39m start)\n",
      "\u001b[1;32m/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb Cell 11\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(all_possible_params, fn_train, params_fn)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*********************************** REP(GA) \u001b[39m\u001b[39m\"\u001b[39m,(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Train and get accuracy for solutions.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m train_population(pop,fn_train,params_fn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Get the average accuracy for this generation.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m average_accuracy \u001b[39m=\u001b[39m get_average_score(pop)\n",
      "\u001b[1;32m/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_population\u001b[0;34m(pop, fn_train, params_fn)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, thread \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(threads):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/d/PFE/Code/CI-Failing-Builds-Detection/GA-HPO-of-LSTM.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m pbar\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "global columns_res,columns_comp\n",
    "columns_res = [\"proj\"]+[\"algo\"]+[\"iter\"]+[\"AUC\"]+[\"accuracy\"]+[\"F1\"]+[\"exp\"]\n",
    "\n",
    "tuner = \"ga\"\n",
    "results = pd.DataFrame(columns =  columns_res)\n",
    "results_train = pd.DataFrame(columns =  columns_res)\n",
    "bellwether=\"steve.csv\"\n",
    "trainset = getDataset_2(bellwether,\"train\")\n",
    "for iteration in range (1,nbr_rep):\n",
    "    entry_train  = evaluate_tuner(tuner,trainset)\n",
    "    best_params = entry_train[\"params\"]\n",
    "    best_model = entry_train[\"model\"]\n",
    "    print(iteration,\"*************************************** TRAIN\",bellwether)\n",
    "    entry_train[\"iter\"] = iteration\n",
    "    entry_train[\"proj\"] = bellwether\n",
    "    entry_train[\"algo\"] = \"LSTM\"\n",
    "    entry_train[\"params\"] = best_params\n",
    "    results_train = results_train.append(entry_train,ignore_index=True)\n",
    "    print(\"entry_train\",entry_train)\n",
    "    for file_name in os.listdir(\"ordered-data\"):\n",
    "        if file_name!=bellwether:\n",
    "            #print(file_name)\n",
    "            testset = getDataset_2(file_name,\"test\")\n",
    "            X,y = test_preprocess(trainset,testset,best_params[\"time_step\"])\n",
    "            entry= predict_lstm(best_model,X,y)\n",
    "            entry[\"iter\"] = iteration\n",
    "            entry[\"proj\"] = file_name\n",
    "            entry[\"exp\"] =  1\n",
    "            entry[\"algo\"] = \"LSTM\"\n",
    "            results = results.append(entry,ignore_index=True)\n",
    "#results.to_excel(\"corss_proj_paramf_\"+str(hybrid_option)+str(with_smote)+\"_result_crossProj_\"+tuner+\"_LSTM.xlsx\")\n",
    "results_train.to_excel(\"cross_paramf\"+str(hybrid_option)+str(with_smote)+\"_train_crossProj_\"+tuner+\"_LSTM.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
