{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JvPXR-dglv1c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal, MultivariateNormal\n",
        "\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import argparse\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UtpIy8-Wlyj5"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, threshold_vector_size, seed, hidden_size=32, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            threshold_vector_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            hidden_size (int): Number of nodes in hidden layers\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)     \n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, threshold_vector_size)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state), inplace=True)\n",
        "        x = F.relu(self.fc2(x), inplace=True)\n",
        "        out = self.fc3(x)\n",
        "        return out\n",
        "\n",
        "    \n",
        "    def get_threshold_vector(self, state):\n",
        "        \"\"\"\n",
        "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
        "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
        "        \"\"\"\n",
        "        #state = torch.FloatTensor(state).to(device) #.unsqzeeze(0)\n",
        "        #action = torch.clamp(action*action_high, action_low, action_high)\n",
        "        \n",
        "        threshold_vector = self.forward(state)\n",
        "        return threshold_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oBuGtzpomnrn"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, threshold_vector_size, number_of_attributes, seed, hidden_size=32):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            threshold_vector_size (int): Dimension of each threshold vector\n",
        "            seed (int): Random seed\n",
        "            hidden_size (int): Number of nodes in the network layers\n",
        "\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size+threshold_vector_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, number_of_attributes)\n",
        "\n",
        "    def forward(self, state, threshold_vector):\n",
        "        \"\"\"Build a critic (value) network that maps (state, threshold_vector) pairs -> Q-values.\"\"\"\n",
        "        x = torch.cat((state, threshold_vector), dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "    def get_attribute(self, threshold_vector):\n",
        "        index_selected_attribute = np.argmax(self.forward(threshold_vector)) # check argmax output\n",
        "        return index_selected_attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TAr3AwlnXEW"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, threshold_vector_size, number_of_attributes, random_seed, hidden_size):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            threshold_vector_size (int): dimension of each threshold vector\n",
        "            random_seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.threshold_vector_size = threshold_vector_size\n",
        "        self.number_of_attributes = number_of_attributes\n",
        "        self.seed = random.seed(random_seed)\n",
        "        \n",
        "        print(\"Using: \", device)\n",
        "        \n",
        "        # actor Network \n",
        "        self.actor_local = Actor(state_size, threshold_vector_size, random_seed, hidden_size).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)     \n",
        "        \n",
        "        # critic Network  \n",
        "        self.critic = Critic(state_size, threshold_vector_size, number_of_attributes, random_seed, hidden_size).to(device)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC, weight_decay=0)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(threshold_vector_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
        "        \n",
        "\n",
        "    def step(self, state, action, reward, next_state, done, step):\n",
        "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
        "        # Save experience / reward\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn, if enough samples are available in memory\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(step, experiences, GAMMA)\n",
        "            \n",
        "    \n",
        "    def act(self, state):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        \n",
        "        # add greedy epsilon\n",
        "\n",
        "        threshold_vector = self.actor_local.get_threshold_vector(state)\n",
        "        index_selected_attribute = self.critic.get_attribute(threshold_vector)\n",
        "        \n",
        "        action = (index_selected_attribute, threshold_vector[index_selected_attribute])\n",
        "\n",
        "        # update tree\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, step, experiences, gamma):\n",
        "        \"\"\"Updates actor, critics using given batch of experience tuples.\n",
        "        Critic_loss = \n",
        "        Actor_loss = \n",
        "        where:\n",
        "            actor_target(state) -> action\n",
        "            critic_target(state, action) -> Q-value\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        \n",
        "\n",
        "        # ---------------------------- update critic ---------------------------- #\n",
        "        # Compute loss\n",
        "        critic_loss = \n",
        "        # Minimize the loss\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # ---------------------------- update actor ---------------------------- #\n",
        "        # Compute loss\n",
        "        actor_loss = \n",
        "\n",
        "        # Minimize the loss\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfaXCEH-nbvN"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        \n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZPj5hK5nfiU"
      },
      "outputs": [],
      "source": [
        "def RLCI(n_episodes=200, max_t=500, print_every=10):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    average_100_scores = []\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "\n",
        "        state = env.reset()\n",
        "        state = state.reshape((1,state_size))\n",
        "\n",
        "        avg_score = 0\n",
        "        for t in range(max_t):\n",
        "\n",
        "            action = agent.act(state)\n",
        "            \n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = next_state.reshape((1,state_size))\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done, t)\n",
        "            state = next_state\n",
        "            avg_score += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        avg_score /= max_t\n",
        "\n",
        "        scores_deque.append(avg_score)\n",
        "        writer.add_scalar(\"Reward\", avg_score, i_episode)\n",
        "        \n",
        "        print('\\rEpisode {} Score: {:.2f}'.format(i_episode, avg_score, end=\"\")\n",
        "        if i_episode % print_every == 0:\n",
        "            print('\\rEpisode {} Score: {:.2f}'.format(i_episode, avg_score, end=\"\")            \n",
        "            \n",
        "    torch.save(agent.actor_local.state_dict(), args.info + \".pt\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "mCsp4HmwnivF",
        "outputId": "db3c656d-729d-4f68-b978-1916d415d421"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "n_episodes = 100\n",
        "GAMMA = 0.99\n",
        "TAU = 1e-2\n",
        "HIDDEN_SIZE = 256\n",
        "BUFFER_SIZE = int(1e6)\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "\n",
        "t0 = time.time()\n",
        "writer = SummaryWriter(\"runs/\")\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_size = \n",
        "threshold_vector_size = \n",
        "number_of_attributes = \n",
        "\n",
        "#env = gym.make(env_name)\n",
        "\n",
        "agent = Agent(state_size=state_size, threshold_vector_size=threshold_vector_size, number_of_attributes=number_of_attributes, random_seed=seed,hidden_size=HIDDEN_SIZE, action_prior=\"uniform\") #\"normal\"\n",
        "\n",
        "RLCI(n_episodes=n_episodes, max_t=500, print_every=10)\n",
        "\n",
        "t1 = time.time()\n",
        "print(\"training took {} min!\".format((t1-t0)/60))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI50xnf_ovVD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RLCIskip.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
