{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df=pd.DataFrame(\n",
        "    columns = ['build_Failed', 'gh_is_pr', 'git_prev_commit_resolution_status',\n",
        "       'gh_team_size', 'gh_num_commit_comments', 'git_diff_src_churn',\n",
        "       'git_diff_test_churn', 'gh_diff_files_added', 'gh_diff_files_deleted',\n",
        "       'gh_diff_files_modified', 'gh_diff_tests_added',\n",
        "       'gh_diff_tests_deleted', 'gh_diff_src_files', 'gh_diff_doc_files',\n",
        "       'gh_diff_other_files', 'gh_sloc', 'gh_test_lines_per_kloc',\n",
        "       'gh_test_cases_per_kloc', 'gh_asserts_cases_per_kloc', 'tr_build_id',\n",
        "       'gh_build_started_at'],\n",
        "    dtype='object')\n",
        "\n",
        "path = 'D:\\PFE\\Code\\CI-Failing-Builds-Detection\\dataset'\n",
        "path = '/mnt/d/PFE/Code/CI-Failing-Builds-Detection/dataset'\n",
        "\n",
        "for dirname, _, filenames in os.walk(path):\n",
        "    for filename in filenames:\n",
        "        if filename[-4:]==\".csv\":\n",
        "            df = pd.concat([df, pd.read_csv(os.path.join(dirname, filename))])\n",
        "\n",
        "X = df.iloc[:,1:19]\n",
        "y = df.iloc[:,0].astype(int)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val , y_train, y_val = train_test_split(np.array(X), np.array(y), test_size=0.2, shuffle=True, stratify=y, random_state=42) # keep ratio of classes in split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modifiable Decision Tree model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.tree import _tree\n",
        "from sklearn.metrics import f1_score \n",
        "\n",
        "class modDecisionTree:\n",
        "    \"\"\"\n",
        "    Represents the classification model\n",
        "    based on sklearn implementation with added methods for modifying single nodes\n",
        "    nodes are indexed depth first\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=3, random_state=42):\n",
        "        # need to add init of hyper-param\n",
        "        self.max_depth = max_depth\n",
        "        self.model = DecisionTreeClassifier(max_depth=max_depth, splitter='random', random_state=random_state)\n",
        "    \n",
        "    def fit(self, X_train, y_train, columns_names):\n",
        "        self.model.fit(X_train, y_train)\n",
        "        self.tree = self.model.tree_\n",
        "        self.n_nodes = self.tree.node_count       # nbr of nodes\n",
        "        self.features = self.tree.feature         # list of each node's feature\n",
        "        self.thresholds = self.tree.threshold     # list of each node's threshold\n",
        "        self.columns_names = columns_names\n",
        "        self.features_names = [list(self.columns_names)[i] for i in self.features]\n",
        "        self.nodes_type = self.get_nodes_type()  # a node is either a split node or a terminal node/leaf\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        y_pred = self.model.predict(X_val)\n",
        "        metrics = {}\n",
        "        metrics['F1'] = f1_score(y_pred ,y_val)\n",
        "        return metrics\n",
        "\n",
        "    def feature_importance(self):\n",
        "        feat_imp = []\n",
        "        for name, importance in zip(self.features_names , self.model.feature_importances_):\n",
        "            feat_imp.append((name, importance))\n",
        "        feat_imp.sort(key=lambda t:t[1], reverse=True)\n",
        "        return feat_imp\n",
        "\n",
        "    def plot_tree(self):\n",
        "        plt.figure(figsize=(15,10))  # set plot size (denoted in inches)\n",
        "        tree.plot_tree(self.model, fontsize=10, class_names=['pass','fail'])\n",
        "        plt.show()\n",
        "    \n",
        "    def get_nodes_type(self):\n",
        "        children_left = self.tree.children_left\n",
        "        children_right = self.tree.children_right\n",
        "        node_depth = np.zeros(shape=self.n_nodes, dtype=np.int64)\n",
        "        is_leaves = np.zeros(shape=self.n_nodes, dtype=bool)\n",
        "        stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
        "        while len(stack) > 0:\n",
        "            # 'pop' ensures each node is only visited once\n",
        "            node_id, depth = stack.pop()\n",
        "            node_depth[node_id] = depth\n",
        "            # If the left and right child of a node is not the same we have a split node\n",
        "            is_split_node = children_left[node_id] != children_right[node_id]\n",
        "            # If a split node, append left and right children and depth to `stack`\n",
        "            # so we can loop through them\n",
        "            if is_split_node:\n",
        "                stack.append((children_left[node_id], depth + 1))\n",
        "                stack.append((children_right[node_id], depth + 1))\n",
        "            else:\n",
        "                is_leaves[node_id] = True\n",
        "        return is_leaves\n",
        "\n",
        "    def node_is_leaf(self, node):\n",
        "        \"\"\"Returns whether a node is a leaf (terminal node) or a split node\"\"\"\n",
        "        return self.nodes_type[node]==1\n",
        "\n",
        "    def set_node_threshold(self, node, value):\n",
        "        if node>=self.tree.node_count:\n",
        "            print(\"Error: selected node id is not in the tree.\")\n",
        "            return\n",
        "        if self.node_is_leaf(node):\n",
        "            print(\"Error: can't change a leaf node's threshold.\")\n",
        "            return\n",
        "        self.threshold[node] = value \n",
        "\n",
        "    def set_node_feature(self, node, feat_index=None, feat_name=None):\n",
        "        if node>=self.tree.node_count:\n",
        "            print(\"Error: selected node id is not in the tree.\")\n",
        "            return\n",
        "        if self.node_is_leaf(node):\n",
        "            print(\"Error: can't change a terminal node's feature.\")\n",
        "            return\n",
        "        # convert feature name to index if supplied with name\n",
        "        if feat_index==None:\n",
        "            feat_index = self.features_names.index(feat_name)\n",
        "        self.features[node] = feat_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### State Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tree_convolution(model, features, thresholds, node=0):\n",
        "    \"\"\"\n",
        "        convolution of each 3 nodes, with overlapping => child of 1 subtree is parent of the next\n",
        "        => after each conv, slit nodes' featuress and thresholdss are updated and terminal nodes are removed\n",
        "        returns new features,thresholds values as lists\n",
        "    \"\"\"\n",
        "    visited = []\n",
        "    queue = []\n",
        "    visited.append(node)\n",
        "    queue.append(node)\n",
        "    new_features = features\n",
        "    new_thresholds = thresholds\n",
        "    #replacing leaf default values -2 by 0 for conv\n",
        "    for node in range(model.n_nodes): \n",
        "        if new_features[node]==-2:\n",
        "            new_features[node]=0\n",
        "        if new_thresholds[node]==-2:\n",
        "            new_thresholds[node]=0\n",
        "    # BFS traversal\n",
        "    while queue:\n",
        "        node = queue.pop(0) \n",
        "        left_node = model.tree.children_left[node]\n",
        "        right_node = model.tree.children_right[node]\n",
        "        #print (node, left_node, right_node, end = \"\\n\")\n",
        "        if left_node not in visited:\n",
        "            visited.append(left_node)\n",
        "            queue.append(left_node)\n",
        "        if right_node not in visited:\n",
        "            visited.append(right_node)\n",
        "            queue.append(right_node)\n",
        "\n",
        "        # define kernel here\n",
        "        if features[left_node]==0 and features[right_node]==0: # both child nodes are leaves\n",
        "            new_node_features = 0\n",
        "            new_node_thresholds = 0\n",
        "        elif features[left_node]==0: # left node is leaf\n",
        "            new_node_features = (features[node]+features[right_node])/2\n",
        "            new_node_thresholds = (thresholds[node]+thresholds[right_node])/2\n",
        "        elif features[right_node]==0: # right node is leaf\n",
        "            new_node_features = (features[node]+features[left_node])/2\n",
        "            new_node_thresholds = (thresholds[node]+thresholds[left_node])/2\n",
        "        else:\n",
        "            new_node_features = (features[node]+features[left_node]+features[right_node])/3\n",
        "            new_node_thresholds = (thresholds[node]+thresholds[left_node]+thresholds[right_node])/3\n",
        "        # save new values\n",
        "        new_features[node] = new_node_features # will auto convert to int \n",
        "        new_thresholds[node] = new_node_thresholds\n",
        "\n",
        "    return new_features, new_thresholds\n",
        "\n",
        "import torch\n",
        "def flatten(features, thresholds):\n",
        "    # removing null values\n",
        "    clean_feat, clean_thres = [], []\n",
        "    for i in range(len(features)):\n",
        "        if features[i]!=0:\n",
        "            clean_feat.append(features[i])\n",
        "            clean_thres.append(thresholds[i])\n",
        "    return torch.cat((torch.FloatTensor(clean_feat), torch.FloatTensor(clean_thres)))\n",
        "\n",
        "\n",
        "def generate_state(model, features, thresholds, nbr_of_conv):\n",
        "    features, thresholds = features.copy(), thresholds.copy()\n",
        "    for _ in range(nbr_of_conv):\n",
        "        new_features, new_thresholds = tree_convolution(model, features, thresholds) # model must be fitted => add condition?\n",
        "        features, thresholds = new_features, new_thresholds\n",
        "    return flatten(features, thresholds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JvPXR-dglv1c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import argparse\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UtpIy8-Wlyj5"
      },
      "outputs": [],
      "source": [
        "class ThresholdsNetwork(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, threshold_vector_size, seed, hidden_size=32):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            threshold_vector_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            hidden_size (int): Number of nodes in hidden layers\n",
        "        \"\"\"\n",
        "        super(ThresholdsNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)     \n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, threshold_vector_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state), inplace=True)\n",
        "        x = F.relu(self.fc2(x), inplace=True)\n",
        "        out = self.fc3(x)\n",
        "        return out\n",
        "    \n",
        "    def get_thresholds_vector(self, state):\n",
        "        \"\"\"\n",
        "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
        "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
        "        \"\"\"\n",
        "        #state = torch.FloatTensor(state).to(device) #.unsqzeeze(0)\n",
        "        #action = torch.clamp(action*action_high, action_low, action_high)\n",
        "        \n",
        "        threshold_vector = self.forward(state)\n",
        "        return threshold_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reinforcement Learning Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oBuGtzpomnrn"
      },
      "outputs": [],
      "source": [
        "class AttributeNetwork(nn.Module):\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, threshold_vector_size, number_of_attributes, seed, hidden_size=32):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            threshold_vector_size (int): Dimension of each threshold vector\n",
        "            seed (int): Random seed\n",
        "            hidden_size (int): Number of nodes in the network layers\n",
        "\n",
        "        \"\"\"\n",
        "        super(AttributeNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size+threshold_vector_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, number_of_attributes)\n",
        "\n",
        "    def forward(self, state, threshold_vector):\n",
        "        \"\"\"Build a critic (value) network that maps (state, threshold_vector) pairs -> Q-values.\"\"\"\n",
        "        x = torch.cat((state, threshold_vector), dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "    def get_attribute(self, state, threshold_vector):\n",
        "        index_selected_attribute = np.argmax(self.forward(state, threshold_vector)) # check argmax output\n",
        "        return index_selected_attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TAr3AwlnXEW"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, threshold_vector_size, number_of_attributes, random_seed, hidden_size):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            threshold_vector_size (int): dimension of each threshold vector\n",
        "            random_seed (int): random seed\n",
        "            add rest\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.threshold_vector_size = threshold_vector_size\n",
        "        self.number_of_attributes = number_of_attributes\n",
        "        self.seed = random.seed(random_seed)\n",
        "        print(\"Using: \", device)\n",
        "\n",
        "        # actor Network \n",
        "        self.ThresholdsNetwork = ThresholdsNetwork(state_size, threshold_vector_size, random_seed, hidden_size).to(device)\n",
        "        self.optimizer_ThresholdsNetwork = optim.Adam(self.ThresholdsNetwork.parameters(), lr=LR_ACTOR)     \n",
        "        \n",
        "        # critic Network  \n",
        "        self.AttributeNetwork = AttributeNetwork(state_size, threshold_vector_size, number_of_attributes, random_seed, hidden_size).to(device)\n",
        "        self.optimizer_AttributeNetwork = optim.Adam(self.AttributeNetwork.parameters(), lr=LR_CRITIC, weight_decay=0)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
        "        \n",
        "    \n",
        "    def act(self, state, eps=0.2):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        # greedy epsilon with param eps\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        thresholds_vector = self.ThresholdsNetwork.get_thresholds_vector(state)\n",
        "\n",
        "        p = np.random.random() \n",
        "        if p<eps:\n",
        "            index_selected_attribute = random.choice(self.number_of_attributes)\n",
        "        else:\n",
        "            index_selected_attribute = self.AttributeNetwork.get_attribute(state, thresholds_vector)\n",
        "        action = (index_selected_attribute, thresholds_vector[index_selected_attribute])\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done, step):\n",
        "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
        "        # Save experience / reward\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # if enough samples are available in memory\n",
        "        # sample a minibatch and learn/update networks\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(step, experiences, GAMMA)\n",
        "\n",
        "    def learn(self, step, experiences, gamma):\n",
        "        \"\"\"Updates actor, critics using given batch of experience tuples.\n",
        "        Critic_loss = \n",
        "        Actor_loss = \n",
        "        where:\n",
        "            actor_target(state) -> action\n",
        "            critic_target(state, action) -> Q-value\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        Q_loss, X_loss = [], []\n",
        "        for experience in experiences: # for b in B\n",
        "            state, action, reward, next_state, done = experience\n",
        "            sb = state\n",
        "            k_act = action[0]\n",
        "            rb = reward\n",
        "            sb1 = next_state # S_{b+1}\n",
        "            Xb = self.ThresholdsNetwork.get_thresholds_vector(sb) # X_{b}\n",
        "            Xb1 = self.ThresholdsNetwork.get_thresholds_vector(sb1) # X_{b+1}\n",
        "            # get max_k(Qq)\n",
        "            Qq = []\n",
        "            for k in range(self.number_of_attributes):\n",
        "                # get Xe_{b+1}\n",
        "                Xeb1k = torch.zeros(len(Xb1))\n",
        "                Xeb1k[k] = Xb1[k] \n",
        "                qek = self.AttributeNetwork.get_attribute(sb1,Xeb1k)\n",
        "                Qq.append(qek)\n",
        "            maxQq = np.max(Qq)\n",
        "            # yb calc\n",
        "            if done: # terminal node\n",
        "                yb = rb\n",
        "            else:\n",
        "                yb = rb + gamma*maxQq\n",
        "\n",
        "            # compute losses for single transitions\n",
        "            # Q loss\n",
        "            xebk = torch.zeros(len(Xb))\n",
        "            xebk[k_act] = Xb\n",
        "            Q_loss.append(yb-self.AttributeNetwork.get_attribute(sb,xebk))\n",
        "            # X loss\n",
        "            sum_Qq = 0\n",
        "            for k in range(self.number_of_attributes):\n",
        "                Xebk = torch.zeros(len(Xb))\n",
        "                Xebk[k] = Xb[k] \n",
        "                # Xe_{b,k}\n",
        "                qek = self.AttributeNetwork.get_attribute(sb1,Xebk)\n",
        "                sul_Qq += qek\n",
        "            X_loss.append(-sum_Qq)\n",
        "\n",
        "        # compute losses as expectation over the experiences batch and update networks\n",
        "\n",
        "        # update thresholds network\n",
        "        # Compute loss\n",
        "        loss_thresholds_network = np.mean(Q_loss)\n",
        "        # Minimize the loss\n",
        "        self.optimizer_ThresholdsNetwork.zero_grad()\n",
        "        loss_thresholds_network.backward()\n",
        "        self.optimizer_ThresholdsNetwork.step()\n",
        "\n",
        "        # update attribute network\n",
        "        # Compute loss\n",
        "        loss_attribute_network = np.mean(X_loss)\n",
        "        # Minimize the loss\n",
        "        self.optimizer_AttributeNetwork.zero_grad()\n",
        "        loss_attribute_network.backward()\n",
        "        self.optimizer_AttributeNetwork.step()\n",
        "\n",
        "\n",
        "def env_step(model, node, action):\n",
        "    # update tree\n",
        "    model.set_node_feature(node, feat_index=action[0])\n",
        "    model.set_node_threshold(node, value=action[1])\n",
        "    next_state = generate_state(model, model.features, model.thresholds, nbr_of_conv)\n",
        "\n",
        "    metrics = model.evaluate(X_val, y_val)\n",
        "    reward = metrics['F1']\n",
        "\n",
        "    done = 0\n",
        "    if model.node_is_leaf(node):\n",
        "        done = 1\n",
        "        \n",
        "    info = 0\n",
        "\n",
        "    return next_state, reward, done, info "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfaXCEH-nbvN"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        \n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "mCsp4HmwnivF",
        "outputId": "db3c656d-729d-4f68-b978-1916d415d421"
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n",
        "\n",
        "seed = 42\n",
        "GAMMA = 0.99\n",
        "TAU = 1e-2\n",
        "HIDDEN_SIZE = 256\n",
        "BUFFER_SIZE = int(1e6)\n",
        "BATCH_SIZE = 256\n",
        "LR_ACTOR = 5e-4\n",
        "LR_CRITIC = 5e-4\n",
        "\n",
        "nbr_of_conv = 2\n",
        "n_episodes=200\n",
        "max_t=500\n",
        "print_every=10\n",
        "\n",
        "scores_deque = deque(maxlen=100)\n",
        "writer = SummaryWriter(\"runs/\")\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "max_depth = 10\n",
        "model = modDecisionTree(max_depth=max_depth)\n",
        "model.fit(X_train, y_train, df.columns)\n",
        "\n",
        "state_size = generate_state(model, model.features, model.thresholds,0).__len__()\n",
        "number_of_attributes = X_train.shape[1]\n",
        "threshold_vector_size = X_train.shape[1]\n",
        "\n",
        "agent = Agent(state_size=state_size, threshold_vector_size=threshold_vector_size, number_of_attributes=number_of_attributes, random_seed=seed,hidden_size=HIDDEN_SIZE, action_prior=\"uniform\") #\"normal\"\n",
        "\n",
        "\n",
        "for i_episode in range(1, n_episodes+1):\n",
        "    # state reset\n",
        "    model = modDecisionTree(max_depth=max_depth)\n",
        "    model.fit(X_train, y_train, df.columns)\n",
        "    state = generate_state(model, model.features, model.thresholds, nbr_of_conv)\n",
        "    #state = state.reshape((1,state_size))\n",
        "    avg_score = 0\n",
        "    for t in range(max_t):\n",
        "        action = agent.act(state, node=t)\n",
        "        next_state, reward, done, info = env_step(model, t, action)\n",
        "        next_state = next_state.reshape((1,state_size))\n",
        "        agent.step(state, action, reward, next_state, done, t)\n",
        "        state = next_state\n",
        "        avg_score += reward\n",
        "        if done:\n",
        "            break\n",
        "    avg_score /= max_t\n",
        "\n",
        "    scores_deque.append(avg_score)\n",
        "    writer.add_scalar(\"Reward\", avg_score, i_episode)\n",
        "    print('\\rEpisode {} Score: {:.2f}'.format(i_episode, avg_score, end=\"\"))\n",
        "    if i_episode % print_every == 0:\n",
        "        print('\\rEpisode {} Score: {:.2f}'.format(i_episode, avg_score, end=\"\"))         \n",
        "        \n",
        "torch.save(agent.actor_local.state_dict(), info + \".pt\")\n",
        "\n",
        "t1 = time.time()\n",
        "print(\"training took {} min!\".format((t1-t0)/60))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RLCIskip.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
