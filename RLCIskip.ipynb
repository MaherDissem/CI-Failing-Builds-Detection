{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JvPXR-dglv1c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal, MultivariateNormal\n",
        "\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import argparse\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UtpIy8-Wlyj5"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, threshold_vector_size, seed, hidden_size=32, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            threshold_vector_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            hidden_size (int): Number of nodes in hidden layers\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)     \n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, threshold_vector_size)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state), inplace=True)\n",
        "        x = F.relu(self.fc2(x), inplace=True)\n",
        "        out = self.fc3(x)\n",
        "        return out\n",
        "\n",
        "    \n",
        "    def get_threshold_vector(self, state):\n",
        "        \"\"\"\n",
        "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
        "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
        "        \"\"\"\n",
        "        #state = torch.FloatTensor(state).to(device) #.unsqzeeze(0)\n",
        "        #action = torch.clamp(action*action_high, action_low, action_high)\n",
        "        \n",
        "        threshold_vector = self.forward(state)\n",
        "        return threshold_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oBuGtzpomnrn"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, threshold_vector_size, number_of_attributes, seed, hidden_size=32):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            threshold_vector_size (int): Dimension of each threshold vector\n",
        "            seed (int): Random seed\n",
        "            hidden_size (int): Number of nodes in the network layers\n",
        "\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size+threshold_vector_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, number_of_attributes)\n",
        "\n",
        "    def forward(self, state, threshold_vector):\n",
        "        \"\"\"Build a critic (value) network that maps (state, threshold_vector) pairs -> Q-values.\"\"\"\n",
        "        x = torch.cat((state, threshold_vector), dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "    def get_attribute(self, threshold_vector):\n",
        "        index_selected_attribute = np.argmax(self.forward(threshold_vector)) # check argmax output\n",
        "        return index_selected_attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TAr3AwlnXEW"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, threshold_vector_size, number_of_attributes, random_seed, hidden_size):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            threshold_vector_size (int): dimension of each threshold vector\n",
        "            random_seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.threshold_vector_size = threshold_vector_size\n",
        "        self.number_of_attributes = number_of_attributes\n",
        "        self.seed = random.seed(random_seed)\n",
        "        \n",
        "        print(\"Using: \", device)\n",
        "        \n",
        "        # actor Network \n",
        "        self.actor_local = Actor(state_size, threshold_vector_size, random_seed, hidden_size).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)     \n",
        "        \n",
        "        # critic Network  \n",
        "        self.critic = Critic(state_size, threshold_vector_size, number_of_attributes, random_seed, hidden_size).to(device)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC, weight_decay=0)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(threshold_vector_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
        "        \n",
        "\n",
        "    def step(self, state, action, reward, next_state, done, step):\n",
        "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
        "        # Save experience / reward\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn, if enough samples are available in memory\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(step, experiences, GAMMA)\n",
        "            \n",
        "    \n",
        "    def act(self, state):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        \n",
        "        # add greedy epsilon\n",
        "\n",
        "        threshold_vector = self.actor_local.get_threshold_vector(state)\n",
        "        index_selected_attribute = self.critic.get_attribute(threshold_vector)\n",
        "        \n",
        "        action = (index_selected_attribute, threshold_vector[index_selected_attribute])\n",
        "\n",
        "        # update tree\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, step, experiences, gamma):\n",
        "        \"\"\"Updates actor, critics using given batch of experience tuples.\n",
        "        Critic_loss = \n",
        "        Actor_loss = \n",
        "        where:\n",
        "            actor_target(state) -> action\n",
        "            critic_target(state, action) -> Q-value\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        \n",
        "\n",
        "        # ---------------------------- update critic ---------------------------- #\n",
        "        # Compute loss\n",
        "        critic_loss = \n",
        "        # Minimize the loss\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # ---------------------------- update actor ---------------------------- #\n",
        "        # Compute loss\n",
        "        actor_loss = \n",
        "\n",
        "        # Minimize the loss\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfaXCEH-nbvN"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        \n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZPj5hK5nfiU"
      },
      "outputs": [],
      "source": [
        "def RLCI(n_episodes=200, max_t=500, print_every=10):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    average_100_scores = []\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "\n",
        "        state = env.reset()\n",
        "        state = state.reshape((1,state_size))\n",
        "\n",
        "        avg_score = 0\n",
        "        for t in range(max_t):\n",
        "\n",
        "            action = agent.act(state)\n",
        "            \n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = next_state.reshape((1,state_size))\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done, t)\n",
        "            state = next_state\n",
        "            avg_score += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        avg_score /= max_t\n",
        "\n",
        "        scores_deque.append(avg_score)\n",
        "        writer.add_scalar(\"Reward\", avg_score, i_episode)\n",
        "        \n",
        "        print('\\rEpisode {} Score: {:.2f}'.format(i_episode, avg_score, end=\"\")\n",
        "        if i_episode % print_every == 0:\n",
        "            print('\\rEpisode {} Score: {:.2f}'.format(i_episode, avg_score, end=\"\")            \n",
        "            \n",
        "    torch.save(agent.actor_local.state_dict(), args.info + \".pt\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "mCsp4HmwnivF",
        "outputId": "db3c656d-729d-4f68-b978-1916d415d421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using:  cpu\n",
            "\rEpisode 1 Reward: -1736.27  Average100 Score: -1736.27"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-05cd31b49188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mSAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-4338d1d16e49>\u001b[0m in \u001b[0;36mSAC\u001b[0;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e3a951832b51>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done, step)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e3a951832b51>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, step, experiences, gamma, d)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# critic 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic1_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mcritic1_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic1_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# critic 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "seed = 42\n",
        "n_episodes = 100\n",
        "GAMMA = 0.99\n",
        "TAU = 1e-2\n",
        "HIDDEN_SIZE = 256\n",
        "BUFFER_SIZE = int(1e6)\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "\n",
        "t0 = time.time()\n",
        "writer = SummaryWriter(\"runs/\")\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_size = \n",
        "threshold_vector_size = \n",
        "number_of_attributes = \n",
        "\n",
        "#env = gym.make(env_name)\n",
        "\n",
        "agent = Agent(state_size=state_size, threshold_vector_size=threshold_vector_size, number_of_attributes=number_of_attributes, random_seed=seed,hidden_size=HIDDEN_SIZE, action_prior=\"uniform\") #\"normal\"\n",
        "\n",
        "RLCI(n_episodes=n_episodes, max_t=500, print_every=10)\n",
        "\n",
        "t1 = time.time()\n",
        "print(\"training took {} min!\".format((t1-t0)/60))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI50xnf_ovVD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RLCIskip.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
